[["index.html", "Human Genome Variation Lab 1 Homepage", " Human Genome Variation Lab 1 Homepage This is the course homepage for Human Genome Variation with Computational Lab (AS.020.321). 1.0.0.1 Instructor Rajiv McCoy, rajiv.mccoy[at]jhu.edu 1.0.0.2 Schedule &amp; Logistics Class is Tuesdays from 3-3:50PM, in UTL G89. Please bring your laptop with you to every class. If you don’t have a working laptop, contact me so that we can find a solution. Session Content Session 1: January 24, 2023 The reference genome Genome browsers Session 2: January 31, 2023 De novo mutations Data exploration and plotting in R Session 3: February 7, 2023 De novo mutations Data exploration and plotting in R Session 4: February 14, 2023 Linkage disequilibrium Session 5: February 21, 2023 Population structure – part I Session 6: February 28, 2023 Population structure – part II Session 7: March 7, 2023 Genome-wide association studies – part I Session 8: March 14, 2023 Genome-wide association studies – part II Spring break – no class Session 9: March 28, 2023 Scans for selection – part I Session 10: April 4, 2023 Archaic admixture Session 11: April 11, 2023 Simulating evolution – part I Session 12: April 18, 2023 Simulating evolution – part II Session 13: April 25, 2023 TBD "],["course-description.html", "1.1 Course Description", " 1.1 Course Description The course on Human Genome Variation has exposed you to the power of genomic studies for understanding human evolutionary history as well as revealing the genetic basis of human traits and disease. What does real human genomic data look like? How are these data analyzed in practice? Supplementing the main course, this lab module will explore public datasets and computational tools used to analyze human genomic data to better understand how patterns in these data can be used to test hypotheses about evolution and human phenotypes. 1.1.0.1 Educational Objectives Explore the ways in which human genomic data is generated, encoded, summarized, and visualized. Develop an awareness of potential confounding factors and approaches by which they can be overcome. Establish familiarity working with summarized forms of genomic data in R, as well as resources for further independent learning. "],["assessment-grading.html", "1.2 Assessment &amp; Grading", " 1.2 Assessment &amp; Grading All students will conduct mid-term and final self-evaluations and use these evaluations for self-grading. The instructor will provide individual feedback on these evaluations. Final grades will be determined either through: A comprehensive final exam that is graded by the instructors A self-grade that is based upon criteria set forth in your self-evaluations. To qualify for self-grading, students must demonstrate participation in 12 out of 13 class sessions. Participation can be demonstrated by in-person class attendance or, if absent from class, via evidence of work on the Posit Cloud workspace associated with a particular week. 1.2.0.1 Homework There are required and optional homework assignments for each topic we cover. "],["posit-cloud.html", "1.3 Posit Cloud", " 1.3 Posit Cloud All the coding and lectures for this class are conducted on Posit Cloud. Posit is a computing environment for R, one of the most widely used coding languages in genomics and statistics. Posit Cloud is an online version of the Posit environment. This cloud workspace avoids us having to install R on everyone’s computers, and also allows us to upload and share data files for class. 1.3.0.1 Making a Posit Cloud account Go to the Posit Cloud website and sign up for an account, using your JHU email. "],["genome-browsers.html", "2 Genome browsers", " 2 Genome browsers In this module, we’ll learn how to use the UC Santa Cruz (UCSC) Genome Browser and the Integrative Genomics Viewer (IGV), two extremely popular tools for visualizing genomic data. 2.0.0.1 Learning objectives After completing this chapter, you’ll be able to: Explain why a reference genome is an important resource for genomics research. Use the UCSC genome browser to find genomic features in a region of interest. Describe the data contained in a file of sequencing reads. Load and interpret sequencing data in IGV. "],["dna-sequencing-data.html", "2.1 DNA sequencing data", " 2.1 DNA sequencing data These days, the vast majority of genomic data is generated through high-throughput Illumina short-read sequencing. The broad steps of this sequencing process are: Extract DNA Fragment DNA Prepare for sequencer (add adapters, etc.) Amplify DNA Sequencing (add fluorescently labeled nucleotides that are read by a digital camera) Fig. 1 (source). Schematic of Illumina short-read sequencing. This sequencing approach is fast and cost efficient, but introduces two main limitations. Because of the fragmentation step, the resulting sequencing reads are extremely short (~150 bp). We don’t know where in the genome the sequencing reads came from. (This is a limitation common to nearly every sequencing experiment.) "],["assembling-a-genome.html", "2.2 Assembling a genome", " 2.2 Assembling a genome When the human genome is 3 billion base pairs long, assembling short sequencing reads into a full genome is a major computational challenge. How is genome assembly performed? We can combine sequencing reads that partially overlap with each other into longer sequences. Fig. 2. Using overlapping sequencing reads to assemble a genome. Which regions of the genome are hardest to assemble? Ideally, with enough sequencing data, we would be able to reconstruct an entire genome from overlapping reads. In practice, genome assembly is complicated by repetitive DNA – sequences in different regions of the genome that are completely or nearly identical. These repeats make it difficult (or impossible) to determine the order of the sequences around them, or how many copies of the repeat there are. Fig. 3. How repetitive sequences affect genome assembly. Resolving repetitive regions requires sequencing reads that are longer than the repeat itself, which allow us to determine the flanking sequences on the sides of the repeat. Using such long-read sequencing technology (i.e., PacBio and Nanopore sequencing), the Telomere-to-Telomere consortium was able to create a complete, ungapped assembly of the human genome in 2021. "],["the-human-reference-genome.html", "2.3 The human reference genome", " 2.3 The human reference genome Having to assemble an entire genome every time you sequence a new individual is a hassle (and often infeasible, if you don’t have enough sequencing data). Instead, we typically align sequencing reads to a reference genome – a high-quality genome assembly for that species, which we use to guide our analysis. The human reference genome was initially assembled in 2000 by the Human Genome Project, and has undergone decades of refinement since. The current version of this reference, which we’ll be using, is hg38. Whose DNA was sequenced for the human reference genome? DNA from multiple individuals was sequenced to construct the reference genome. Its sequence is a mosaic of these individuals’ DNA. You can classify the ancestry of different parts of hg38 by comparing its sequence to DNA from different populations. From this, we know that around 70% of hg38 comes from one individual, called RP11, who likely had African American ancestry. Fig. 4. Sample composition of the human reference genome. "],["ucsc-genome-browser.html", "2.4 UCSC genome browser", " 2.4 UCSC genome browser What does the human reference genome actually look like? We can view it in the UC Santa Cruz (UCSC) genome browser, an interactive website for viewing genomes – both the human reference and reference genomes for several other species. The browser also displays genome annotations, such as the locations of genes and clinically relevant genetic variants. Go to the UCSC genome browser. 2.4.1 Homepage There are a few key areas of this page: Browse/Select Species – choose the species Human Assembly – choose the version of the human reference genome Position/Search Term – type in a specific position (ex: chr2:25160915), region (ex: chr1:100000-200000), or gene name (ex: HLA-A) Fig. 5. UCSC genome browser homepage. In Position/Search Term, type in a gene you’re interested in and hit enter. Why are there different versions of the reference genome? You may have noticed that the name of the reference genome we’re using is GRCh38 or hg38, which stands for Genome Reference Consortium Human Build 38 – version 38 of the reference genome. Over time, the Genome Reference Consortium makes improvements to the reference genome by closing gaps, fixing problems, and resolving repetitive regions. hg38, the most recent version, was released in 2013. "],["viewing-one-region-of-the-genome.html", "2.5 Viewing one region of the genome", " 2.5 Viewing one region of the genome Once you hit enter, you should end up on a page like this: Fig. 6. Viewing one genomic region in the UCSC genome browser. The default display includes these broad groups of annotations: Navigation: Buttons for zooming and moving around (you can also move by clicking the display, holding, and moving your mouse); current region; search bar Position: Current position on the chromosome; current base pair position Genes: Gene annotations; gene expression by tissue; gene regulatory elements (CREs) Species comparison: DNA sequence conservation across vertebrates; regions that align with the genomes of other vertebrates Variation: Genetic variants in the dbSNP database; repeat elements Inspecting a specific track If you’re interested in more information about a specific track – for example, the POMC gene annotation – you can click on that element to go to a webpage with more details. Fig. 7. Clicking on the POMC gene track. (Note that if you click on whitespace instead of an annotation element, it will change the track’s display density instead.) Fig. 8. Clicking on whitespace to expand the RefSeq genes track. Customizing the display tracks The tracks that are automatically displayed are just a small subset of what’s available. You can select which tracks you want to see, and set their display density, by scrolling down on the page. To add a new track to your browser view, click the drop-down menu below that track and select any of the options besides hide. Here we’re viewing the “Clone Ends” track, which shows the different individuals that were sequenced to create this section of the reference genome. Fig. 9. Adding the “clone ends” track to the browser. Click the refresh button in the upper right to reload the genome view. You should see something like this, showing that this region of the reference genome was sequenced in three individuals (CH17, CTD, and RP11): Fig. 10. Viewing the “clone ends” track. "],["igv.html", "2.6 IGV", " 2.6 IGV While the UCSC genome browser lets you view the reference genome itself, we’re often interested in looking at sequencing data – sequencing reads that are aligned to the reference genome. For this we use the Integrative Genomics Viewer (IGV). Go to the IGV web app. 2.6.1 Homepage The IGV homepage is fairly empty because we haven’t loaded any sequencing reads to look at, and also because we’re zoomed too far out to see anything. The Genome tab in the upper right lets you choose which reference genome to work in. The default is hg38 A drop-down menu and search bar below the header allow you to pick a chromosome and genomic position Fig. 11. The IGV homepage. We haven’t chosen a chromosome yet, so all of them are displayed below the drop-down menu. Click on one to go to a zoomed-in view of that particular chromosome. "],["navigating-igv.html", "2.7 Navigating IGV", " 2.7 Navigating IGV Once you’ve clicked on a chromosome, zoom in until you can see colors on the top track. This track displays the DNA sequence, colored by nucleotide. The track below the DNA sequence has gene annotations from RefSeq. Fig. 12. Viewing a gene in IGV. "],["loading-sequencing-data.html", "2.8 Loading sequencing data", " 2.8 Loading sequencing data Click on the Genome drop-down menu and switch to the Human (hg38 1kg/GATK) reference genome. This version of the hg38 reference has sequencing data already loaded into the IGV web app. Once you’ve switched references, click Tracks -&gt; 1KG Low Coverage Alignments. This gives you a list of sample to load sequencing data from. Click any sample and then OK. Fig. 13. Loading reads from a 1000 Genomes sample. "],["the-1000-genomes-project.html", "2.9 The 1000 Genomes Project", " 2.9 The 1000 Genomes Project Where did this sequencing data come from? In 2015, a study called the 1000 Genomes Project (1KG or 1KGP) sequenced 3,202 individuals from 26 globally diverse populations. Because this data is publicly available, it’s become one of the most widely used datasets in human genetics. Notably, 1KGP still excludes key regions of the world – such as Oceania, the Middle East, native American populations in North America, and many populations within Africa. Fig. 14. Regions sampled by the 1000 Genomes Project. Go to the 1000 Genomes Project website and click the Data tab. Then click the link to the data portal. Fig. 15. The data portal includes information about the samples in this dataset. Choose any individual and copy their sample ID (ex: HG00138). We can use this sample ID to find this individual’s raw sequencing data in the Sequence Read Archive (SRA). "],["sra.html", "2.10 SRA", " 2.10 SRA Search for the sample ID you chose in SRA. You should see something like this, where every item is a sequencing dataset generated for this sample. Fig. 16. Finding sequencing data in SRA. 2.10.1 Previewing sequencing data Choose any sequencing dataset, and then click on any item in the Run table at the bottom. This takes you to a page that displays a specific sequencing run (i.e., one use of a sequencing machine). Go to the Reads tab. Fig. 17. The Reads tab in SRA. The right-hand panel shows one sequencing read from this run. Note that the sequence of this read is around 100bp long – the average length for short-read Illumina sequencing. All sequencing data looks like this! It’s just a text file filled with the IDs and DNA/RNA sequences of your reads. "],["viewing-sequencing-reads-in-igv.html", "2.11 Viewing sequencing reads in IGV", " 2.11 Viewing sequencing reads in IGV Now that we’ve seen what raw sequencing data looks like, let’s look at it in IGV. Return to your IGV tab, where you should have one sample’s DNA sequencing data loaded. Make sure you’re zoomed in enough for the data to display. Fig. 18. The Reads tab in SRA. The top track is a histogram of sequencing coverage (i.e., how many reads there are at that position in the genome). The bottom track shows the reads themselves. How do we know where in the genome each read belongs? We match the sequence of the read to the sequence of the reference genome (called alignment). With 100bp reads, the probability that a match occurs by chance is \\(\\frac{1}{4^{100}}\\), or \\(6.2 * 10^{-61}\\). Extracting alignment information in IGV If you click on a specific read, IGV will display additional information about it, including: The exact position it aligns to The mapping quality (a score indicating how uniquely it aligns to this position) If you’re working with paired-end sequencing data, where its paired read is Fig. 19. Viewing additional info for one sequencing read. "],["interpreting-igv-alignments.html", "2.12 Interpreting IGV alignments", " 2.12 Interpreting IGV alignments Sequencing reads in IGV are colored at bases where they differ from the reference genome. These differences can be caused by either real genetic variation or sequencing error. How would you distinguish these two? Fig. 20. Two of these colored bases are probably real SNPs, and two are probably errors. The sequencing coverage track also colors the positions that it thinks are real variants. In the screenshot above, which spans about 2kb, there are two SNPsin the coverage track. This pattern holds more broadly through the genome – humans carry about one SNP every 1,000 bases. Is one SNP every 1,000bp a lot or a little? Humans actually have much lower amounts of genetic variation than many species, including many of the great apes. This is mostly the result of human evolutionary history. Because the effective size of human populations has historically been low, with only very recent expansion, the gene pool is still fairly homogenous, with many rare variants and few common ones. "],["conclusion.html", "2.13 Conclusion", " 2.13 Conclusion In this lab, we explored several of the most commonly used websites in genomics: 2.13.0.1 Genome browsers UCSC genome browser: Used to explore features of the human genome If you discover an interesting SNP in your research, you might look it up in the UCSC browser to see which genes it’s in/near, if it overlaps with any repetitive elements, etc. IGV: Used to visualize sequencing data It’s common practice to look at your sequencing reads in IGV to check alignment quality, verify that SNPs look like real variants and not errors, etc. 2.13.0.2 Data repositories 1000 Genomes Project: One of the largest and most diverse datasets of human sequencing data Data from 1000 Genomes is frequently used in human genetics studies SRA: A repository for publicly available sequencing data Genetics studies deposit their data in SRA if it can be made publicly available (i.e., if it has no identifiable information) "],["homework-1.html", "2.14 Homework", " 2.14 Homework 2.14.0.1 Goals &amp; Learning Objectives The goal of this homework is to make an account in Posit Cloud to use for the rest of the semester. 2.14.1 Required homework Follow the instructions here to create a Posit Cloud account and join the HGV workspace. "],["discovering-mutations.html", "3 Discovering mutations", " 3 Discovering mutations In this module, we’ll use DNA sequencing data from human families to explore the relationship between parental age and de novo mutations in their children. 3.0.0.1 Learning objectives After completing this chapter, you’ll be able to: Create plots to visualize the relationship between two variables. Interpret the results of a linear model. Compare the impact of maternal vs. paternal age on de novo mutation counts. Explain what a confidence interval is and why it’s useful. "],["de-novo-mutations.html", "3.1 De novo mutations", " 3.1 De novo mutations Mutation and recombination are two biological processes that generate genetic variation. When these phenomena occur during gametogenesis, the changes that they make to DNA are passed down to the next generation through germline cells (i.e., sperm and oocyte). De novo mutations (DNMs) arise from errors in DNA replication or repair. These mutations can be single-nucleotide polymorphisms (SNPs) or insertions and deletions of DNA. Every individual typically carries around 70 de novo SNPs that were not present in either of their parents. Fig. 1. Sources of DNMs in gametogenesis. "],["recombination.html", "3.2 Recombination", " 3.2 Recombination Crossovers, or meiotic recombination, occur during prophase of meiosis I, when homologous chromosomes pair with each other. Double-strand breaks are deliberately generated in the DNA, and are then cut back and repaired based on the sequence of the homologous chromosome. These repairs can sometimes resolve in a crossover event, where sections of DNA are swapped between chromosomes. Because the sequences of homologous chromosomes differ at sites where they carry different alleles, recombination generates genetic diversity by creating new haplotypes, or combinations of alleles. Crossovers are required for meiosis in most organisms because they ensure proper homologous chromosome pairing and segregation. Humans experience 1-4 crossover events per chromosome, with longer chromosomes having more crossovers. Fig. 2. Possible outcomes for double-strand breaks generated during meiosis I. Adapted from Molecular Biology of the Cell, 6th Edition (Alberts et al.) "],["setup.html", "3.3 Setup", " 3.3 Setup In this module, we’ll use sequencing data from families to look at the relationship between DNMs, crossovers, and parental age. 3.3.1 R packages We’re using R’s tidyverse library to analyze our data. You can load this R package by running: library(tidyverse) 3.3.2 Data Our data comes from the supplementary tables of this paper by Halldorsson et al., which performed whole-genome sequencing on “trios” (two parents and one child) in Iceland. We’ve pre-processed the data to make it easier to work with. Load the pre-processed data by running the code chunk below. # read data dnm_by_age &lt;- read.table(&quot;dnm_by_age_tidy_Halldorsson.tsv&quot;, sep = &quot;\\t&quot;, header = TRUE) # preview data head(dnm_by_age) ## Proband_id n_paternal_dnm n_maternal_dnm n_na_dnm Father_age Mother_age ## 1 675 51 19 0 31 36 ## 2 1097 26 12 1 19 19 ## 3 1230 42 12 3 30 28 ## 4 1481 53 14 1 32 20 ## 5 1806 61 11 6 38 34 ## 6 2280 63 9 3 38 20 The columns in this table are: Proband_id: ID of the child (i.e., “proband”) n_paternal_dnm: Number of DNMs (carried by the child) that came from the father n_maternal_dnm: Number of DNMs that came from the mother n_na_dnm: Number of DNMs whose parental origin can’t be determined Father_age: Father’s age at proband’s birth Mother_age: Mother’s age at proband’s birth "],["visualizing-the-data.html", "3.4 Visualizing the data", " 3.4 Visualizing the data We can use our tidied data to ask questions about the de novo mutation rate in these Icelandic individuals. How does parental age affect the number of DNMs for males and females? Use the dnm_by_age data to plot this relationship for males. ggplot(data = dnm_by_age, # specify where ggplot should be getting the x location for each data point aes(x = Father_age, # specify where ggplot should be getting the y location for each data point y = n_paternal_dnm)) + # specify that the data should be plotted as points geom_point() Based on your plot, would you say that there’s an association between paternal age and number of DNMs? It looks like there’s a pretty strong association between paternal age and number of DNMs, where older males have more DNMs. Modify your code to plot the relationship between age and number of DNMs for females. Does there seem to be an association between maternal age and number of DNMs? ggplot(data = dnm_by_age, aes(x = Mother_age, y = n_maternal_dnm)) + geom_point() There’s also a strong positive association between maternal age and number of DNMs, although the slope (i.e., the increase in number of DNMs per year) is shallower. "],["linear-models.html", "3.5 Linear models", " 3.5 Linear models We can visually observe that age seems associated with number of DNMs in both males and females, but we need a way to ask if that this is a statistically meaningful association. We can do this with a linear model. This model fits a line to the plots that we just made, and asks if the slope is significantly different from 0 (i.e., if there’s a significant increase in DNM count as age increases). If this is a statistical test, what’s the null hypothesis? The null hypothesis for this linear model is that the slope is 0 – i.e., that there’s no association between parental age and the number of DNMs from that parent. If the slope is significantly different from 0, we can reject the null hypothesis. We’ll fit a linear model using R’s lm function. Run the following code block to open a manual describing the function. ?lm lm requires two arguments: The formula or equation it’s evaluating A table of data The formula must be in the format response variable ~ predictor variable(s), where each variable is the name of a column in our data table. Is our predictor variable the parental age or the number of DNMs? The predictor variable is parental age. We expect the number of DNMs to change as a consequence of parental age. "],["fitting-a-linear-model-for-dnms.html", "3.6 Fitting a linear model for DNMs", " 3.6 Fitting a linear model for DNMs Run the following code to fit a model for the effect of age on paternal DNMs. # fit linear model for paternal DNMs fit_pat &lt;- lm(formula = n_paternal_dnm ~ Father_age, data = dnm_by_age) # print results of model summary(fit_pat) ## ## Call: ## lm(formula = n_paternal_dnm ~ Father_age, data = dnm_by_age) ## ## Residuals: ## Min 1Q Median 3Q Max ## -32.785 -5.683 -0.581 5.071 31.639 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.58819 1.70402 6.214 1.34e-09 *** ## Father_age 1.34849 0.05359 25.161 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.426 on 388 degrees of freedom ## Multiple R-squared: 0.62, Adjusted R-squared: 0.619 ## F-statistic: 633.1 on 1 and 388 DF, p-value: &lt; 2.2e-16 How do you interpret results from a linear model? For our purposes, the only part of the results you need to look at is the line under (Intercept) in the Coefficients section: Estimate Std. Error t value Pr(&gt;|t|) Father_age 1.34849 0.05359 25.161 &lt; 2e-16 *** The fourth columm, Pr(&gt;|t|), is the p-value. Because this p-value is &lt; 2e-16, we can reject the null hypothesis and say that there is association between paternal age and the number of paternal DNMs. The first column, Estimate, is the slope, or coefficient. Linear regression fits a line to our plot of paternal age vs. number of DNMs. The coefficient estimate is the slope of that line. The slope for paternal age given by this linear model is 1.34849. We can interpret this number this way: For every additional year of paternal age, we expect 1.35 additional paternal DNMs in the child. Modify your code to assess the relationship between maternal age and number of maternal DNMs. Is this relationship significant? How many maternal DNMs do we expect for every additional year of maternal age? # fit linear model for maternal DNMs fit_mat &lt;- lm(formula = n_maternal_dnm ~ Mother_age, data = dnm_by_age) # print results of model summary(fit_mat) ## ## Call: ## lm(formula = n_maternal_dnm ~ Mother_age, data = dnm_by_age) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.8683 -3.1044 -0.2329 2.2394 17.5379 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.51442 0.98193 2.561 0.0108 * ## Mother_age 0.37846 0.03509 10.785 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.503 on 388 degrees of freedom ## Multiple R-squared: 0.2307, Adjusted R-squared: 0.2287 ## F-statistic: 116.3 on 1 and 388 DF, p-value: &lt; 2.2e-16 The p-value is &lt;2e-16 and the Mother_age slope is 0.37846. This relationship is significant, and we expect 0.38 more maternal DNMs for every additional year of maternal age. "],["confidence-intervals.html", "3.7 Confidence intervals", " 3.7 Confidence intervals Our models predict that there are 1.35 more DNMs for additional every year of paternal age, and 0.38 more DNMs for every additional year of maternal age. Does this mean that sperm and oocytes accumulate DNMs at different rates? The maternal and paternal slopes look different, but we need statistical evidence that they actually are. (For example, what if there’s a lot of variability in the maternal DNM data, and the true maternal coefficient could be anywhere between -1 and 10?) To do this, we compare the confidence intervals of our slope estimates. What is a confidence interval? We use confidence intervals when estimating a value – in this case, the Mother_age and Father_age slope parameters. A confidence interval (CI) is a random interval that has a 95% probability of falling on the parameter we are estimating. So, a 95% CI contains the true value of the slope 95% of the time. Keep in mind that the definition above (95% of random intervals fall on the true value) is not the same as saying there is a 95% chance that the true value falls within our interval. This latter statement is not accurate. In R, we get the confidence interval of a parameter from a linear model with the confint function. ?confint confint requires three arguments: A fitted linear model (our fit_pat variable) The parameter we want a CI for (Father_age) The CI’s probability (typically 95%) "],["calculate-95-cis.html", "3.8 Calculate 95% CIs", " 3.8 Calculate 95% CIs Run the following code to calculate the 95% confidence interval for the Father_age slope parameter. confint(fit_pat, &#39;Father_age&#39;, level = 0.95) ## 2.5 % 97.5 % ## Father_age 1.243118 1.45386 So, 95% of the time, the number of additional DNMs per year of paternal age is between 1.24 and 1.45. Modify your code to get the 95% CI for the Mother_age slope. What’s the interpretation of this confidence interval? confint(fit_mat, &#39;Mother_age&#39;, level = 0.95) ## 2.5 % 97.5 % ## Mother_age 0.3094713 0.4474528 95% of the time, the number of additional DNMs per year of maternal age is between 0.31 and 0.45. Now that we have the confidence intervals for both slope parameters, we can finally compare them. Our two CI ranges are non-overlapping. The paternal range is [1.24, 1.45] and the maternal range is [0.31, 0.45]. If the 95% CIs for two parameters don’t overlap, this strongly supports that the parameters are significantly different from one another. So, it seems likely that paternal and maternal gametes experience different rates of de novo mutation. If the CIs for two parameters overlap, are they not significantly different? Not necessarily. More analysis, like a hypothesis test, is needed to make a final decision. "],["conclusion-1.html", "3.9 Conclusion", " 3.9 Conclusion In this lab, we explored the relationship between parental age and the number of de novo mutations in their gametes. We plotted the relationship between maternal/paternal age and DNM count. This visualization suggested that DNM count increases with age for both groups. We confirmed this hypothesis by using a linear model, which tests if additional years of age have a non-zero effect on the number of DNMs. The number of paternal DNMs seemed to increase more quickly with age than maternal DNMs. We confirmed this by comparing the 95% confidence intervals of the slopes of the two models. One final question – let’s assume that there really is a difference between the effect of age on DNMs in male and female gametes. What biological reasons might be causing this difference? "],["homework-2.html", "3.10 Homework", " 3.10 Homework So far, we’ve only looked at the de novo mutation data from the Halldorsson et al. paper. Now we’ll use their data on the number of maternal and paternal origin crossovers (i.e., how many crossovers occurred across all chromosomes in the maternal and paternal gametes). 3.10.0.1 Goals &amp; Learning Objectives The goal of this homework is to practice with ggplot. Learning Objectives Required homework: Practice visualizing data with ggplot2 Optional homework: Practice interpreting linear models "],["required-homework-1.html", "3.11 Required homework", " 3.11 Required homework The data from the paper has been pre-filtered for you. Run this code block to read it in: # read data crossovers &lt;- read.table(&quot;crossovers.tsv&quot;, header = TRUE) # preview data head(crossovers) ## Proband_id n_pat_xover n_mat_xover Father_age Mother_age ## 1 3 22 51 29 28 ## 2 10 26 50 26 26 ## 3 11 25 38 25 22 ## 4 15 24 50 31 26 ## 5 20 27 35 26 24 ## 6 22 28 40 39 31 The columns in this table are: Proband_id: ID of the child n_pat_xover: Number of crossovers (carried by the child) that occurred in the paternal gametes n_mat_xover: Number of crossovers that occurred in the maternal gametes Father_age: Father’s age at proband’s birth Mother_age: Mother’s age at proband’s birth Assignment: Using the ggplot code from this module, plot the relationship between parental age and number of crossovers. As with the DNM data, make one plot for the maternal crossovers and one plot for the paternal. Do you think parental age impacts crossover number? Solution Plot paternal crossovers: ggplot(data = crossovers, # x axis is paternal age aes(x = Father_age, # y axis is number of crossovers y = n_pat_xover)) + geom_point() Plot maternal crossovers: ggplot(data = crossovers, # x axis is maternal age aes(x = Mother_age, # y axis is number of crossovers y = n_mat_xover)) + geom_point() Just by eye, it doesn’t really seem that age affects number of crossovers for either mothers or fathers. "],["optional-homework.html", "3.12 Optional homework", " 3.12 Optional homework Assignment: Fit two linear models (one paternal, one maternal) to ask if there is an association between the number of parental crossovers and parental age. If there is an association, how is the number of crossovers predicted to change with every year of maternal/paternal age? Solution # fit the model with paternal age fit_pat &lt;- lm(data = crossovers, formula = n_pat_xover ~ Father_age) summary(fit_pat) ## ## Call: ## lm(formula = n_pat_xover ~ Father_age, data = crossovers) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.2173 -3.1880 -0.1997 2.8061 24.7652 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 26.369432 0.102736 256.67 &lt;2e-16 *** ## Father_age -0.005852 0.003462 -1.69 0.091 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.388 on 41090 degrees of freedom ## Multiple R-squared: 6.953e-05, Adjusted R-squared: 4.519e-05 ## F-statistic: 2.857 on 1 and 41090 DF, p-value: 0.09098 There isn’t a significant association between paternal age and the number of paternal crossovers (p = 0.091). # fit the model with maternal age fit_mat &lt;- lm(data = crossovers, formula = n_mat_xover ~ Mother_age) summary(fit_mat) ## ## Call: ## lm(formula = n_mat_xover ~ Mother_age, data = crossovers) ## ## Residuals: ## Min 1Q Median 3Q Max ## -27.161 -6.095 -0.425 5.641 45.905 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 41.709271 0.206238 202.24 &lt;2e-16 *** ## Mother_age 0.065989 0.007576 8.71 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.685 on 41090 degrees of freedom ## Multiple R-squared: 0.001843, Adjusted R-squared: 0.001819 ## F-statistic: 75.87 on 1 and 41090 DF, p-value: &lt; 2.2e-16 Surprisingly, there is a significant association between maternal age and the number of maternal crossovers (p &lt; 2e-16). For every year of maternal age, we expect the child to carry 0.07 additional maternal origin crossovers. Although the maternal crossovers plot doesn’t look that impressive, our estimated slope is 0.07, which is probably too small to distinguish visually. "],["linkage-disequilibrium.html", "4 Linkage disequilibrium", " 4 Linkage disequilibrium In this module, we’ll use DNA sequencing data from human populations to assess linkage disequilibrium between two genetic variants. 4.0.0.1 Learning objectives After completing this chapter, you’ll be able to: Define linkage disequilibrium. Manually calculate \\(D\\), \\(D&#39;\\), and \\(r^2\\) from genotype data. Understand the differences in interpretation for different LD statistics. Explain how LD both benefits and limits genetics studies. "],["what-is-linkage-disequilibrium.html", "4.1 What is linkage disequilibrium?", " 4.1 What is linkage disequilibrium? Linkage disequlibrium (LD) refers to correlation among genotypes at multiple sites in the genome. This is a consequence of the fact that chromosomes are transmitted in “chunks” through the generations. When mutations arise, they arise on a single chromosome with a given set of alleles. The new mutation will continue to be associated with this genetic background until it is shuffled during the process of meiotic recombination. Together, a set of alleles that tend to occur together because of linkage disequilibrium is called a haplotype. Fig. 1. After a new mutation arises, recombination over the course of many generations reduces the number of variants in LD with it. "],["why-do-we-care-about-ld.html", "4.2 Why do we care about LD?", " 4.2 Why do we care about LD? As a result of linkage disequilibrium, knowledge of a genotype at one site in the genome can provide information about the genotype at another site, even if the second site was not actually genotyped. Using prior knowledge of LD to “fill in” missing genotype information is a process called imputation. Linkage disequilibrium also means that correlation between a genotype at a particular site and phenotype (e.g., disease outcome) does not imply causation. Even ignoring other possible confounders, any variant on the same haplotype could be driving the association. Beyond mutation and recombination, other evolutionary forces such as gene flow, genetic drift, and natural selection can also influence patterns of LD observed in population genetic data. Measuring linkage disequilibrium is therefore important for both medical and evolutionary studies. Fig. 2. LD can be used to impute missing genotypes, but also complicates genetic association studies (such as finding variants that cause disease). Non-causal variants in LD will perfectly co-occur with the causal variant, making it difficult to determine which one is truly causal. "],["setup-1.html", "4.3 Setup", " 4.3 Setup We’ll measure LD between two SNPs called in the 1000 Genomes Project dataset: rs28574812 (chr21:15012619) rs2251399 (chr21:15013185) We’ve preprocessed the original 1000 Genomes data such that every line in the table below represents one haplotype in the 1000 Genomes database. Load the pre-processed data by running the code below. # read data haplotypes &lt;- read.table(&quot;snp_haplotypes.txt&quot;, header = TRUE) # preview data head(haplotypes) ## sample haplotype snp1_allele snp2_allele ## 1 HG00096 hap_1 A C ## 2 HG00096 hap_2 A C ## 3 HG00097 hap_1 A C ## 4 HG00097 hap_2 A C ## 5 HG00099 hap_1 A C ## 6 HG00099 hap_2 A C The columns in this table are: sample: Name of the individual who was sequenced haplotype: Haplotype (i.e., the maternal or paternal chromosome) that the SNP is on snp1_allele: Genotype at SNP1 on this haplotype snp2_allele: Genotype at SNP2 on this haplotype Note that there are 2,504 samples in the 1000 Genomes Project but 5,008 total lines in the table. This is because there are two lines per individual – one for each of their maternal and paternal haplotypes. Fig. 3. Our reformatted VCF shows the combinations of alleles at two SNPs of interest, for all haplotypes in the 1000 Genomes dataset. "],["are-these-snps-in-ld.html", "4.4 Are these SNPs in LD?", " 4.4 Are these SNPs in LD? If we run table on each SNP column, we can see which alleles exist at SNP1 and SNP2. SNP1 can be A or G SNP2 can be C or T table(haplotypes$snp1_allele) ## ## A G ## 3456 1552 table(haplotypes$snp2_allele) ## ## C T ## 2825 2183 If these two SNPs were in perfect LD, we’d expect to see only two haplotypes in our data (Fig. 4A). A C: If someone carries an A at SNP1, they will always carry a C at SNP2. G T: If they carry a G at SNP1, they will always carry a T at SNP2. If these two SNPs were in linkage equilibrium, the allele at SNP1 would give us no information about SNP2. We would expect to see all four possible haplotypes, in amounts proportional to the component allele frequencies (Fig. 4B). A C A T G C G T Fig. 4. When two SNPs are in perfect LD, seeing an allele on one haplotype perfectly predicts which allele is on the other haplotype. "],["counting-haplotypes-with-table.html", "4.5 Counting haplotypes with table", " 4.5 Counting haplotypes with table We can use the table function to count the occurrence of the four possible haplotypes. table(haplotypes$snp1_allele, haplotypes$snp2_allele) ## ## C T ## A 2655 801 ## G 170 1382 The table tells us that there are 2655 A C haplotypes (A at SNP1 and C at SNP2), 170 G C haplotypes, etc. Do these SNPs look like they’re in LD? It looks like there are some haplotypes (A C and G T) that are overrepresented. However, it’s hard to tell whether that’s just because an A allele at SNP1 is much more common than T is. "],["fishers-exact-test.html", "4.6 Fisher’s exact test", " 4.6 Fisher’s exact test We can wrap our table in the fisher.test function to perform a Fisher’s exact test. This test tells us whether there is a non-random association between any of the SNP alleles, while accounting for the relative proportions of each allele. fisher.test(table(haplotypes$snp1_allele, haplotypes$snp2_allele)) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: table(haplotypes$snp1_allele, haplotypes$snp2_allele) ## p-value &lt; 2.2e-16 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 22.49760 32.33934 ## sample estimates: ## odds ratio ## 26.9124 The p-value is very small (&lt; 2.2e-16), so we can reject the null hypothesis that the two SNPs are associating independently of each other. How do we interpret the odds ratio from the Fisher’s exact test? In addition to the p-value, Fisher’s Exact Test also gives us an odds ratio of 26.9, with a 95% confidence interval of [22.5, 32.3]. In this context, the odds ratio reflects how much more likely you are to see an A at SNP1 if you also see an C at SNP2. We can observe that the 95% confidence interval doesn’t overlap with 1. An odds ratio of 1 would mean that seeing C at SNP2 doesn’t influence the probability of seeing an A at SNP1. This is a somewhat non-canonical usage of Fisher’s exact test, and in our case the odds ratio applies to whatever haplotype is in the top left corner of our 4x4 table. We could rearrange the table to calculate the odds ratio for any of the three other combinations of alleles. Together, the p-value and 95% confidence interval tell us that there is strong evidence of LD between these SNPs. Perhaps this isn’t surprising, since our two SNPs are common and close to one another on chromosome 21. "],["measuring-ld-with-d.html", "4.7 Measuring LD with \\(D\\)", " 4.7 Measuring LD with \\(D\\) If SNP1 and SNP2 are in linkage equilibrium, the probability of seeing an A C haplotype should be equal to the product of the allele frequencies of A and C. This is simply the probablity of observing two events together if the events are independent. Otherwise, for SNPs that are not independent of each other, we should see A C either more or less often than expected from the allele frequencies. This intuition is summarized in \\(\\mathbf{D}\\), a population genetics statistic for measuring LD between two SNPs. \\[ D = h_{pq} - p_1*q_1 \\] \\(\\mathbf{h_{pq}}\\) is the frequency of our haplotype of interest (A C). \\(\\mathbf{p_1*q_1}\\) is the product of the frequencies of the two alleles on this haplotype (A at SNP1 and C at SNP2) How do we interpret \\(D\\)? If two SNPs are in linkage equilibrium, \\(h_{pq}\\) and \\(p_1*q_1\\) should be the same, and we should get \\(D = 0\\). If two SNPs are in linkage disequilibrium, \\(p_1*q_1\\) should be different from \\(h_{pq}\\), so that \\(D \\neq 0\\). "],["calculating-d.html", "4.8 Calculating \\(D\\)", " 4.8 Calculating \\(D\\) We can re-run our table code to find the probabilities we need for calculating \\(D\\). table(haplotypes$snp1_allele, haplotypes$snp2_allele) ## ## C T ## A 2655 801 ## G 170 1382 \\[ D = h_{pq} - p_1*q_1 \\] What are \\(h_{pq}\\), \\(p_1\\), and \\(q_1\\)? \\(h_{pq}\\) is the probability of seeing the A C haplotype. This is equal to the number of A C haplotypes over the number of total haplotypes: \\[\\frac{2655}{2655 + 170 + 801 + 1382} = \\frac{2655}{5008}\\] \\(p_1\\) is the probability that SNP1 is A. We can get this by adding across the first row of the table (i.e., adding the number of A C and A T haplotypes): \\[\\frac{2655 + 801}{5008}\\] \\(q_1\\) is the probability that SNP2 is C. We can get this by adding across the first column of the table (i.e., adding the number of A C and G G haplotypes): \\[\\frac{2655 + 170}{5008}\\] (Note that the denominator is always 5008 – the total number of haplotypes in our dataframe.) Now we can plug in the corresponding probabilities to calculate D: # define our probabilities of interest h &lt;- 2655 / 5008 p1 &lt;- (2655 + 801) / 5008 q1 &lt;- (2655 + 170) / 5008 # calculate D D &lt;- h - (p1 * q1) D ## [1] 0.1408705 \\(D = 0.14\\), which is non-zero, suggesting that these SNPs are in LD. "],["measuring-ld-with-d-1.html", "4.9 Measuring LD with \\(D&#39;\\)", " 4.9 Measuring LD with \\(D&#39;\\) Aside from being nonzero, what does the value of \\(D\\) mean? This is surprisingly hard to interpret because the minimum and maximum value of \\(D\\) is different for every pair of SNPs. Why does the range of \\(D\\) change? The possible values of \\(D\\) depend on the frequencies of the alleles at each SNP. For example: If \\(p_1 = 0.5\\) and \\(q_1 = 0.5\\), then \\(D\\) is between \\([-0.25, 0.25]\\) If \\(p_1 = 0.1\\) and \\(q_1 = 0.7\\), then \\(D\\) is between \\([-0.07, 0.03]\\) The \\(\\mathbf{D&#39;}\\) statistic fixes this issue by dividing \\(D\\) by its theoretical maximum. \\(D&#39;\\) is constrained between \\([-1, 1]\\), where more extreme values denote stronger LD. \\[ D&#39; = \\frac{D}{\\mathrm{max}(-p_1 (1-p_1), -q_1 (1-q_1))}, \\mathrm{\\:for\\:} D &lt; 0 \\\\ D&#39; = \\frac{D}{\\mathrm{min}(p_1 (1-p_1), q_1 (1-q_1))}, \\mathrm{\\:for\\:} D &gt; 0 \\] \\(p_1\\) and \\(p_2\\) are the frequencies of the alleles at SNP1 and SNP2. Use this formula to calculate \\(D&#39;\\) for our two SNPs of interest. Because \\(D\\) is positive, we use the second formula for \\(D&#39;\\). First, we need to find the denominator, which is the minimum of \\(p_1 (1-p_1)\\) and \\(q_1 (1-q_1)\\). p1 * (1-p1) # pmin ## [1] 0.2138636 q1 * (1-q1) # qmin ## [1] 0.2458915 p1 * (1-p1) is smaller, so we plug that into our \\(D&#39;\\) formula: Dprime &lt;- D / (p1 * (1-p1)) Dprime ## [1] 0.6586931 This tells us that LD between these two SNPs is 65.9% of its theoretical maximum. "],["measuring-ld-with-r2.html", "4.10 Measuring LD with \\(r^2\\)", " 4.10 Measuring LD with \\(r^2\\) \\(\\mathbf{r^2}\\) is the most common statistic for measuring LD. Its value ranges from [0, 1], where 1 indicates maximum LD. \\[ r^2 = \\frac{D^2}{p_1 (1-p_1) q_1 (1-q_1)} \\] Although it looks similar to the formulas for \\(D\\) and \\(D&#39;\\), \\(r^2\\) is actually derived from the correlation coefficient of the frequencies of SNP1 and SNP2, and has a slightly different interpretation: \\(D\\) and \\(D&#39;\\) measure whether recombination has occurred between two alleles \\(r^2\\) measures how well we can predict the allele at one locus if given the allele at the other locus Calculate \\(r^2\\) for our two SNPs of interest. r2 &lt;- D^2 / (p1*(1-p1)*q1*(1-q1)) r2 ## [1] 0.3773631 \\(r^2 = 0.38\\), indicating that these SNPs are in moderate LD. "],["ldlink.html", "4.11 LDlink", " 4.11 LDlink LDlink is a web application that allows you to compute and visualize linkage disequilibrium using data from the 1000 Genomes Project (the same dataset we’ve been using for this module). Go to LDlink’s LDpair tool, which computes \\(D&#39;\\) and \\(r^2\\) between pairs of SNPs. Using either the rsIDs or the chromosome and position of the two SNPs we looked at today, check our calculations for \\(D&#39;\\) and \\(r^2\\). Make sure you: Select All Populations, since we didn’t subset our data by population. If using SNP position, note that our data was aligned to the GRCh38 reference genome. Fig. 5. LDpair results for the two SNPs from this class. We can see that these \\(D&#39;\\) and \\(r^2\\) statistics, as well as the 4x4 table, are very similar to what we calculated by hand! (The values aren’t identical because we’re using a slightly different genotyping dataset.) "],["visualizing-ld-blocks.html", "4.12 Visualizing LD blocks", " 4.12 Visualizing LD blocks LDproxy, another LDlink tool, finds all SNPs in strong LD with a SNP of interest. Open LDproxy and use it to search for one of the SNPs from today (it may take the webpage a minute to load the results). Fig. 6. LDproxy results for rs28574812. To generate this plot, LDproxy calculated \\(r^2\\) between our SNP of interest and all other SNPs in a 500kb window. As expected, we can see that LD is strongest for variants that are closest to the SNP. LDproxy also provides even more information than just LD – it also includes regulatory annotations for all the variants in this region (the numbers within each dot), as well as gene annotations and a list of nearby variants (below the plot). "],["ld-in-association-studies.html", "4.13 LD in association studies", " 4.13 LD in association studies The figure below is a locuszoom plot – a common visualization of data from genome-wide association studies (GWAS). This particular study was a GWAS for genetic variants that impact mean corpuscular hemoglobin concentration. The left y-axis is the p-value for the association with mean corpuscular hemoglobin concentration Each variant is colored by its \\(\\mathbf{r^2}\\) with the top hit variant (in purple) The heatmap on the bottom shows pairwise LD between variants The right y-axis and dark blue line show the frequency of recombination events. Peaks are recombination hotspots (note how they line up with the boundaries of LD blocks in the heatmap) We can observe a block of red/orange variants with almost the same p-value as the top hit. All of these variants are in strong LD with each other: all of them except the leftmost cluster fall within the same LD block in the heatmap on the bottom. As a result, any of them could be causal – i.e., the one that actually affects corpuscular hemoglobin. This is a problem that affects all association studies. Two common ways of working around LD to identify causal variants are: Statistical fine mapping: Uses patterns of LD and statistical models to narrow down casual variant sets Experimental screening: Tests candidate variants in vitro (ex: massively parallel reporter assays, CRISPR screens) to determine which have functional effects Fig. 7. GWAS associations with mean corpuscular hemoglobin concentration, from this paper. "],["conclusion-2.html", "4.14 Conclusion", " 4.14 Conclusion In this lab, we used genotype data from the 1000 Genomes Project to ask whether there is linkage disequilibrium between two SNPs on chr21. Using data from the VCF, we used table to count how often we observe combinations of alleles at these SNPs. We used the data in the table to calculate three LD statistics: \\(\\mathbf{D}\\): the deviation of the observed haplotype frequency from the expected haplotype frequency \\(\\mathbf{D&#39;}\\): a normalization of \\(D\\) that ranges from \\([-1, 1]\\) \\(\\mathbf{r^2}\\): how well the allele at one locus predicts the allele at another locus We used LDlink to visualize how blocks of LD define haplotypes. "],["homework-3.html", "4.15 Homework", " 4.15 Homework 4.15.0.1 Goals &amp; Learning Objectives The goal of this homework is to calculate and interpret LD statistics for other sets of SNPs. Learning Objectives Practice calculating and interpreting LD statistics "],["required-homework-2.html", "4.16 Required homework", " 4.16 Required homework We’ve subset the VCF from class to show haplotypes for two different pair of SNPs: chr21:15005329 and chr21:15007704 chr21:13217431 and chr21:13232002 # read data for first set of SNPs hw1 &lt;- read.table(&quot;snp_haplotypes_hw1.txt&quot;, header = TRUE) # read data for second set of SNPs hw2 &lt;- read.table(&quot;snp_haplotypes_hw2.txt&quot;, header = TRUE) Assignment: Using the code from class, calculate \\(D\\), \\(D&#39;\\), and \\(r^2\\) for these sets of SNPs. Which alleles are segregating together? What does each LD statistic indicate? (Feel free to check your work on LDpair.) Solution for first set of SNPs First use table to count the occurences of the four haplotypes. table(hw1$snp1_allele, hw1$snp2_allele) ## ## A G ## C 747 2508 ## T 2 1751 All four possible haplotypes exist in this population. \\(\\mathbf{D = h_{pq} - p_1*q_1}\\) h &lt;- 747 / 5008 p1 &lt;- (747 + 2508)/5008 q1 &lt;- (747 + 2)/5008 D &lt;- h - p1 * q1 D ## [1] 0.05195286 \\(D\\) is non-zero, which suggests that these SNPs might be in LD. \\(\\mathbf{D&#39; = {D}{\\mathrm{min}(p_1 (1-p_1), q_1 (1-q_1))}}\\) (because \\(D &gt; 0\\)) First we determine the denominator by calculating which of \\(p_1 (1-p_1)\\) and \\(q_1 (1-q_1)\\) is smaller: p1 * (1-p1) ## [1] 0.227512 q1 * (1-q1) ## [1] 0.1271923 \\(q1 * (1-q1)\\) is smaller, so we use it for the denominator. \\(D&#39;\\) is: Dprime &lt;- D / (q1 * (1-q1)) Dprime ## [1] 0.4084591 \\(D&#39; = 0.41\\), which suggests low LD (some recombination has occurred between alleles on this haplotype). \\(\\mathbf{r^2 = \\frac{D^2}{p_1 (1-p_1) q_1 (1-q_1)}}\\) r2 &lt;- D^2 / (p1 * (1-p1) * q1 * (1-q1)) r2 ## [1] 0.09327254 However, \\(r^2 = 0.09\\) (linkage equilibrium)! This is because one of the haplotypes, A T, is very rare – there are only two copies in the population. \\(r^2\\) tells us that the counts of the A T haplotype are so low that an A at SNP1 doesn’t do a great job of predicting when SNP2 is T. Solution for second set of SNPs First use table to count the occurences of the four haplotypes. table(hw2$snp1_allele, hw2$snp2_allele) ## ## A G ## A 3522 0 ## G 0 1486 The only haplotypes that exist in this population are A C and G G. \\(\\mathbf{D = h_{pq} - p_1*q_1}\\) h &lt;- 3522 / 5008 p1 &lt;- (3522 + 0)/5008 q1 &lt;- (3522 + 0)/5008 D &lt;- h - p1 * q1 D ## [1] 0.2086794 \\(D\\) is non-zero, which suggests that these SNPs might be in LD. \\(\\mathbf{D&#39; = \\frac{D}{\\mathrm{min}(p_1 (1-p_1), q_1 (1-q_1))}}\\) (because \\(D &gt; 0\\)) First we determine the denominator by calculating which of \\(p_1 (1-p_1)\\) and \\(q_1 (1-q_1)\\) is smaller: p1 * (1-p1) ## [1] 0.2086794 q1 * (1-q1) ## [1] 0.2086794 The two values are exactly the same, so we can use either for the denominator. \\(D&#39;\\) is: Dprime &lt;- D / (p1 * (1-p1)) Dprime ## [1] 1 \\(D&#39; = 1\\)! These SNPs are in maximum LD (no recombination has occured between them). \\(\\mathbf{r^2 = \\frac{D^2}{p_1 (1-p_1) q_1 (1-q_1)}}\\) r2 &lt;- D^2 / (p1 * (1-p1) * q1 * (1-q1)) r2 ## [1] 1 \\(r^2 = 1\\)! These SNPs are in maximum LD (everyone who carries an A at SNP1 has an A at SNP2, and everyone with a G at SNP1 has a G at SNP2). "],["population-structure.html", "5 Population structure", " 5 Population structure In this lab, we’ll implement two common approaches for measuring and visualizing population structure: \\(F_{ST}\\) and principal component analysis (PCA). 5.0.0.1 Learning objectives After completing this chapter, you’ll be able to: Describe the data stored in a Variant Call Format (VCF) file. Plot and interpret an allele frequency spectrum. Perform and visualize the results of a principal component analysis. "],["what-is-a-population.html", "5.1 What is a population?", " 5.1 What is a population? In population genetics, the term population refers to a group of interbreeding individuals. Determining whether a group of individuals is a “population” is subjective – groups exchange migrants at different rates (a process called “gene flow”), and there is no definitive boundary for when they constitute separate populations. 5.1.0.1 What is population structure? Population structure is a consequence of the fact that when two groups of individuals do not freely interbreed, the two populations typically develop different patterns of genetic variation. Individuals within a population tend to be share greater genetic similarity with each other than with individuals in other populations. These differences manifest through differences in allele frequencies among populations, and result from genetic drift, natural selection, and other evolutionary forces. We can measure such allele frequency differences to reveal evolutionary relationships among populations, as well as evidence of historical natural selection. Fig. 1. Two populations polymorphic for alleles A and a. The frequency of A in Population 1 is \\(0.83\\), while its frequency in Population 2 is \\(0.15\\). "],["geography-of-genetic-variants.html", "5.2 Geography of Genetic Variants", " 5.2 Geography of Genetic Variants One quick way to visualize population structure in humans is to look at the allele frequencies of genetic variants in different regions of the world. The Geography of Genetic Variants (GGV) browser is a website that plots allele frequencies from the 1000 Genomes dataset. For a given variant, GGV plots piecharts of its frequency in the 26 populations in 1000 Genomes, superimposed over the population’s geographical location. While some variants have very similar frequencies across populations, others are much more common in specific populations – usually suggesting that these populations are more closely related to each other. Fig. 2. GGV visualization of the allele frequencies for a SNP at chr1:222087833. "],["variant-call-format-vcf.html", "5.3 Variant Call Format (VCF)", " 5.3 Variant Call Format (VCF) We’re investigating population structure in genotype data from the 1000 Genomes Project. We’ll eventually work with an R-specific form of this data, but first let’s look at the full file, which is in Variant Call Format (VCF). 5.3.1 What is a VCF? VCF files store genotype data at variable sites. Every line of a VCF represents a genetic variant, and contains information about what the variant is and which individuals carry it. While these are just text files, they have a strange format that goes beyond a simple table. This is a preview of the first few lines of random_snippet.vcf.gz, which we’ll be working with today: Fig. 3. A preview of random_snippet.vcf.gz. How do you read a VCF file? The first section of a VCF is a multi-line header – marked by the # character – which contains metadata and descriptions of the FILTER, INFO, and FORMAT columns. (For example, the INFO column usually includes information like the variant’s allele frequency, and each field is described in the header.) The final line of the header gives the names of the columns. The data section of a VCF describes its variants and genotypes. The first 8 columns are about the variant itself – its position, the reference/alternative alleles, etc. The rest of the VCF columns contain the genotypes of all the samples it has data for. Here, every column is one individual from the 1000 Genomes Project (so there are 2,504 columns total). How do you interpret VCF genotypes? 0|0: homozygous reference (does not carry the variant) 0|1 or 1|0: heterozygous 1|1: homozygous alternate (both chromosomes have the variant) .: Missing genotype (could not be confidently called) The sample-specific columns often include additional genotype information, like the number of sequencing reads from the individual that support the reference vs. alternative alleles. The included fields are specified in the FORMAT column (which in this case just reads GT, for “genotype”). "],["setup-2.html", "5.4 Setup", " 5.4 Setup 5.4.1 R packages Load these libraries to use for the lab: library(tidyverse) library(vcfR) 5.4.2 Data 5.4.2.1 Genotype data Our genotype data is in a VCF file called random_snippet.vcf.gz, which contains a random subset of variants on chromosome 21 that were genotyped in 1000 Genomes. Use vcfR to read in the data: vcf &lt;- read.vcfR(file = &quot;random_snippet.vcf.gz&quot;) ## Scanning file to determine attributes. ## File attributes: ## meta lines: 19 ## header_line: 20 ## variant count: 10000 ## column count: 2513 ## Meta line 19 read in. ## All meta lines processed. ## gt matrix initialized. ## Character matrix gt created. ## Character matrix gt rows: 10000 ## Character matrix gt cols: 2513 ## skip: 0 ## nrows: 10000 ## row_num: 0 ## Processed variant 1000 Processed variant 2000 Processed variant 3000 Processed variant 4000 Processed variant 5000 Processed variant 6000 Processed variant 7000 Processed variant 8000 Processed variant 9000 Processed variant 10000 Processed variant: 10000 ## All variants processed 5.4.2.2 Metadata 1000 Genomes provides metadata (like population labels) for every sample in their dataset. Read in the integrated_call_samples.txt metadata file: metadata &lt;- read.table(&quot;integrated_call_samples.txt&quot;, header = TRUE) head(metadata) ## sample pop superpop sex ## 1 HG00096 GBR EUR male ## 2 HG00097 GBR EUR female ## 3 HG00099 GBR EUR female ## 4 HG00100 GBR EUR female ## 5 HG00101 GBR EUR male ## 6 HG00102 GBR EUR female The columns of this table are: sample: Sample ID (matches the columns in the VCF file) pop: Population that individual belongs to superpop: Superpopulation – continental groupings of the 1000 Genomes populations. The five superpopulations in this dataset are: AFR: African AMR: Admixed American EAS: East Asian EUR: European SAS: South Asian sex: Sample sex "],["tidying-vcf-data.html", "5.5 Tidying VCF data", " 5.5 Tidying VCF data If we try to look at vcf, we can see that it’s an “Object of Class vcfR”: vcf ## ***** Object of Class vcfR ***** ## 2504 samples ## 1 CHROMs ## 10,000 variants ## Object size: 194.1 Mb ## 0 percent missing data ## ***** ***** ***** vcfR reformats VCFs into its own data class, which is easy to manipulate using vcfR functions, but hard to access directly from the vcf variable. We’ll use the vcfR2tidy function in order to convert vcf into tidy dataframes suitable for analysis with R: # convert vcf into three tidy dataframes tidied &lt;- vcfR2tidy(vcf, # tell vcfR to turn these INFO fields into integers info_types = c(AF = &quot;n&quot;, EAS_AF = &quot;n&quot;, EUR_AF = &quot;n&quot;, AFR_AF = &quot;n&quot;, AMR_AF = &quot;n&quot;, SAS_AF = &quot;n&quot;)) tidied ## $fix ## # A tibble: 10,000 × 20 ## ChromKey CHROM POS ID REF ALT QUAL FILTER AF AC NS AN ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 chr21 1.00e7 &lt;NA&gt; C A NA PASS 0.02 89 2548 5008 ## 2 1 chr21 1.03e7 &lt;NA&gt; A G NA PASS 0.02 84 2548 5008 ## 3 1 chr21 1.03e7 &lt;NA&gt; G A NA PASS 0 3 2548 5008 ## 4 1 chr21 1.03e7 &lt;NA&gt; A C NA PASS 0 1 2548 5008 ## 5 1 chr21 1.03e7 &lt;NA&gt; C A NA PASS 0 3 2548 5008 ## 6 1 chr21 1.03e7 &lt;NA&gt; A T NA PASS 0 4 2548 5008 ## 7 1 chr21 1.03e7 &lt;NA&gt; G A NA PASS 0 4 2548 5008 ## 8 1 chr21 1.03e7 &lt;NA&gt; A G NA PASS 0 7 2548 5008 ## 9 1 chr21 1.03e7 &lt;NA&gt; A C NA PASS 0 1 2548 5008 ## 10 1 chr21 1.03e7 &lt;NA&gt; G C NA PASS 0 1 2548 5008 ## # ℹ 9,990 more rows ## # ℹ 8 more variables: EAS_AF &lt;dbl&gt;, EUR_AF &lt;dbl&gt;, AFR_AF &lt;dbl&gt;, AMR_AF &lt;dbl&gt;, ## # SAS_AF &lt;dbl&gt;, VT &lt;chr&gt;, EX_TARGET &lt;chr&gt;, DP &lt;chr&gt; ## ## $gt ## # A tibble: 25,040,000 × 5 ## ChromKey POS Indiv gt_GT gt_GT_alleles ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 10005999 HG00096 0|0 C|C ## 2 1 10325486 HG00096 0|0 A|A ## 3 1 10336823 HG00096 0|0 G|G ## 4 1 10337236 HG00096 0|0 A|A ## 5 1 10339129 HG00096 0|0 C|C ## 6 1 10339141 HG00096 0|0 A|A ## 7 1 10339175 HG00096 0|0 G|G ## 8 1 10339803 HG00096 0|0 A|A ## 9 1 10339869 HG00096 0|0 A|A ## 10 1 10347083 HG00096 0|0 G|G ## # ℹ 25,039,990 more rows ## ## $meta ## # A tibble: 13 × 5 ## Tag ID Number Type Description ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 INFO AF A Float Estimated allele frequency in the range (0,1) ## 2 INFO AC A Integer Total number of alternate alleles in called … ## 3 INFO NS 1 Integer Number of samples with data ## 4 INFO AN 1 Integer Total number of alleles in called genotypes ## 5 INFO EAS_AF A Float Allele frequency in the EAS populations calc… ## 6 INFO EUR_AF A Float Allele frequency in the EUR populations calc… ## 7 INFO AFR_AF A Float Allele frequency in the AFR populations calc… ## 8 INFO AMR_AF A Float Allele frequency in the AMR populations calc… ## 9 INFO SAS_AF A Float Allele frequency in the SAS populations calc… ## 10 INFO VT . String indicates what type of variant the line repr… ## 11 INFO EX_TARGET 0 Flag indicates whether a variant is within the ex… ## 12 INFO DP 1 Integer Approximate read depth; some reads may have … ## 13 FORMAT gt_GT 1 String Phased Genotype tidied is a list (a type of R object) of three dataframes: fix: Location, identity, allele frequency, etc. of variants in the VCF gt: Genotypes of the VCF samples meta: Information from the VCF header We’ll be using the variants information from tidied, so let’s store it in a separate table: # make variants dataframe variants &lt;- tidied$fix variants ## # A tibble: 10,000 × 20 ## ChromKey CHROM POS ID REF ALT QUAL FILTER AF AC NS AN ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 chr21 1.00e7 &lt;NA&gt; C A NA PASS 0.02 89 2548 5008 ## 2 1 chr21 1.03e7 &lt;NA&gt; A G NA PASS 0.02 84 2548 5008 ## 3 1 chr21 1.03e7 &lt;NA&gt; G A NA PASS 0 3 2548 5008 ## 4 1 chr21 1.03e7 &lt;NA&gt; A C NA PASS 0 1 2548 5008 ## 5 1 chr21 1.03e7 &lt;NA&gt; C A NA PASS 0 3 2548 5008 ## 6 1 chr21 1.03e7 &lt;NA&gt; A T NA PASS 0 4 2548 5008 ## 7 1 chr21 1.03e7 &lt;NA&gt; G A NA PASS 0 4 2548 5008 ## 8 1 chr21 1.03e7 &lt;NA&gt; A G NA PASS 0 7 2548 5008 ## 9 1 chr21 1.03e7 &lt;NA&gt; A C NA PASS 0 1 2548 5008 ## 10 1 chr21 1.03e7 &lt;NA&gt; G C NA PASS 0 1 2548 5008 ## # ℹ 9,990 more rows ## # ℹ 8 more variables: EAS_AF &lt;dbl&gt;, EUR_AF &lt;dbl&gt;, AFR_AF &lt;dbl&gt;, AMR_AF &lt;dbl&gt;, ## # SAS_AF &lt;dbl&gt;, VT &lt;chr&gt;, EX_TARGET &lt;chr&gt;, DP &lt;chr&gt; "],["the-allele-frequency-spectrum.html", "5.6 The allele frequency spectrum", " 5.6 The allele frequency spectrum One common visualization of genotype data is the allele frequency spectrum (AFS), which is the distribution of the allele frequencies (AFs) of the variants. These allele frequencies were calculated in the original VCF, and are now in the AF column of variants. Plot the AFS using the data from variants ggplot(data = variants, aes(x = AF)) + geom_histogram(bins = 100) Interpreting the AFS The allele frequencies in this dataset range from 0 to 1, with an exponential decay from zero, indicating that the majority of variants are rare. This is the expected shape of an AFS, since all variants arise in one individual and are unlikely to spread widely through the population just by chance. This distribution is exacerbated in human populations, where recent population expansions have resulted in an excess of rare variation. "],["theoretical-afs.html", "5.7 Theoretical AFS", " 5.7 Theoretical AFS What does a normal AFS should look? Population geneticists have estimated that under neutral expectations, each bin of the AFS should have a height that is equal to 1 over its bin number. We can use this to plot the expected AFS: # make dataframe with theoretical AFS bins # create `af_bin` column with the bin number ideal_pop &lt;- tibble(af_bin = 1:100) %&gt;% # create `prop` column with the expected proportion of variants mutate(., prop = 1 / af_bin) # plot expected AFS ggplot(ideal_pop, aes(x = af_bin, y = prop)) + geom_bar(stat = &quot;identity&quot;) How does this compare to the AFS we see from human data? The human AFS has many more rare variants, which manifests as a higher peak on the left side of the AFS. This is due to recent population expansion in humans, which results in more human individuals and an accumulation of excess new rare variation. How would you expect the AFS to look for a contracting population (ex: endangered species)? A contracting population would result in the extinction of many alleles, resulting in more variants that drift to high frequency or go extinct. The AFS for this type of population would look more flat than the neutral expectation (fewer rare alleles, more common ones). "],["common-variation.html", "5.8 Common variation", " 5.8 Common variation For this lab, we’ll subset to just common variants within the VCF – arbitrarily defined as \\(0.05 &lt; \\textrm{AF} &lt; 0.95\\). We can look at where this set of variants lies on the AFS by adding vertical lines at the cutoff allele frequencies: ggplot(data = variants, aes(x = AF)) + geom_histogram(bins = 100) + geom_vline(xintercept = 0.05, linetype = &quot;dashed&quot;, color = &quot;blue&quot;) + geom_vline(xintercept = 0.95, linetype = &quot;dashed&quot;, color = &quot;brown&quot;) Why only work with common variants? Rare variants are more likely to show fine-grained population structure – for example, a variant may be carried by just one individual, or just one family. Because there are so many rare variants, including them causes differences between individuals to be more pronounced than differences between populations. While this is a biologically true statement, it makes it harder to visualize population structure, which is why we subset to common variation for PCA. "],["subsetting-to-common-variants.html", "5.9 Subsetting to common variants", " 5.9 Subsetting to common variants Subset the original VCF object to just common variation, using the variants dataframe (which contains the AF information): # choose rows of `variants` that have AFs within range common_rows &lt;- (variants$AF &gt; 0.05) &amp; (variants$AF &lt; 0.95) # subset rows of the vcf object vcf_common &lt;- vcf[common_rows, ] vcf_common ## ***** Object of Class vcfR ***** ## 2504 samples ## 1 CHROMs ## 960 variants ## Object size: 18.8 Mb ## 0 percent missing data ## ***** ***** ***** As expected from the cutoff lines on the AFS plot, there are only 960 variants left after subsetting – substantially less than our original count of 10,000. We’ll also subset the variants dataframe itself to only keep information for the common variants. variants_common &lt;- variants[common_rows, ] Re-plot the AFS of our subsetted data to confirm that we’ve successfully selected common variants. ggplot(data = variants_common, aes(x = AF)) + geom_histogram(bins = 90) + geom_vline(xintercept = 0.05, linetype = &quot;dashed&quot;, color = &quot;blue&quot;) + geom_vline(xintercept = 0.95, linetype = &quot;dashed&quot;, color = &quot;brown&quot;) Our data is correctly subset within the dashed lines. Even with just common variation, we still observe an exponential decay of the allele frequencies. "],["principal-component-analysis.html", "5.10 Principal component analysis", " 5.10 Principal component analysis Principal component analysis (PCA) is a method for representing high-dimensional data (i.e., data with many variables) within a smaller number of dimensions. In our case, every individual in the VCF has genotype measurements for hundreds of variants. You can think of PCA as a projection all the individuals in our dataset into a cloud, where their position is determined by their combination of genotypes. The first principal component (PC) is the vector through the cloud of data points that captures the greatest possible variance. The second PC is the vector that captures the second greatest possible variance, and must also be perpendicular to the first vector. The same idea applies to the third, fourth, fifth, etc. PCs. For an in-depth visual walkthrough of PCA, you can look at this website. Fig. 7. A PCA plot that simplifies three-dimensional data into two dimensions. "],["reformatting-data-for-pca.html", "5.11 Reformatting data for PCA", " 5.11 Reformatting data for PCA We’re using R’s prcomp function to perform PCA on our genotype data. This function takes a matrix where the rows are the data objects (i.e., individuals) and the columns are the associated measurements (i.e., variants). The values within the matrix also have to be numeric. To reformat our data for prcomp, we need to: Extract genotypes from the VCF with vcfR’s extract.gt function Convert the genotypes into numeric values, where 0 is homozygous reference (0|0) 1 is heterozygous (0|1, 1|0) 2 is homozygous alternate (1|1) These values also represent the number of alternative alleles that an individual carries. Transpose (i.e., rotate) the matrix so the rows are samples and the columns are variants We’ll use a tidyverse function to convert, or “recode”, the genotype values in every column of the matrix. # extract genotypes from vcfR object gt_matrix &lt;- extract.gt(vcf_common) %&gt;% # convert to dataframe so we can use tidyverse functions as.data.frame() %&gt;% # apply `recode` function to all values in dataframe mutate_all(recode, &quot;0|0&quot; = 0, &quot;1|0&quot; = 1, &quot;0|1&quot; = 1, &quot;1|1&quot; = 2) %&gt;% # convert back to matrix as.matrix() head(gt_matrix) ## HG00096 HG00097 HG00099 HG00100 HG00101 HG00102 HG00103 HG00104 HG00105 ## [1,] 0 0 1 0 0 1 0 0 1 ## [2,] 1 0 0 0 1 0 0 0 0 ## [3,] 1 0 0 0 1 0 0 0 0 ## [4,] 0 0 0 0 0 0 0 0 0 ## [5,] 0 0 0 0 0 0 0 0 1 ## [6,] 1 0 0 0 0 0 0 0 0 ## HG00106 ## [1,] 0 ## [2,] 0 ## [3,] 0 ## [4,] 0 ## [5,] 0 ## [6,] 0 We transpose the matrix with prcomp’s t function: gt_matrix_T &lt;- t(gt_matrix) head(gt_matrix_T) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## HG00096 0 1 1 0 0 1 0 0 0 0 ## HG00097 0 0 0 0 0 0 0 0 0 0 ## HG00099 1 0 0 0 0 0 0 0 0 0 ## HG00100 0 0 0 0 0 0 0 0 0 0 ## HG00101 0 1 1 0 0 0 1 0 0 0 ## HG00102 1 0 0 0 0 0 0 0 0 0 "],["performing-pca.html", "5.12 Performing PCA", " 5.12 Performing PCA We’re finally ready to PCA our genotype matrix with prcomp. pca &lt;- prcomp(gt_matrix_T) Our output is a prcomp object. Like the tidied vcfR object we worked with earlier in the lab, this prcomp object comprises several tables, which you can preview by typing pca$ into the console and seeing what R suggests: sdev: standard deviations of the principle components rotation, center, scale: tables we won’t use in this lab x: coordinates of the data objects (the 1000 Genomes individuals) on each PC "],["reformatting-pca-output.html", "5.13 Reformatting PCA output", " 5.13 Reformatting PCA output We can plot our PCA output using the information in pca$x. x &lt;- pca$x head(x) ## PC1 PC2 PC3 PC4 PC5 PC6 ## HG00096 3.060014 -5.822356 -1.2683268 -2.95001629 0.46698982 0.1339302 ## HG00097 2.839200 -6.278675 0.8609691 -2.56106805 -1.74037088 1.9513776 ## HG00099 1.803619 -5.171999 0.4033319 -2.55619801 -2.60575981 -2.5503366 ## HG00100 3.160473 -4.504760 1.8926507 -3.90772028 -2.28653995 -1.3882114 ## HG00101 4.035908 -4.545304 0.9407191 -0.04543605 -0.03966489 -0.7168129 ## HG00102 3.608347 -4.668695 0.7327117 -1.65671069 0.36325717 -0.9040767 ## PC7 PC8 PC9 PC10 ## HG00096 -0.9330964 1.4328185 -1.1828901 -0.95170825 ## HG00097 1.4655485 0.4610298 -0.6380021 0.81143912 ## HG00099 0.6484548 0.8349526 0.4990934 -0.22078094 ## HG00100 1.4227095 -0.7013128 -0.1769408 0.36643824 ## HG00101 -2.0747496 0.3429485 0.8956504 -0.03026584 ## HG00102 -1.0028895 -0.6966921 -0.7946498 1.11763306 We can see that every row of x is one individual, every column is a PC (going up to 960 PCs!), and the value in each cell represents the sample’s coordinate on each PC axis. Let’s create a dataframe of the first three PCs to plot: # create column of sample names pca_results &lt;- data.frame(sample = rownames(x), PC1 = x[, 1], # PC1 values PC2 = x[, 2], # PC2 values PC3 = x[, 3]) # PC3 values head(pca_results) ## sample PC1 PC2 PC3 ## HG00096 HG00096 3.060014 -5.822356 -1.2683268 ## HG00097 HG00097 2.839200 -6.278675 0.8609691 ## HG00099 HG00099 1.803619 -5.171999 0.4033319 ## HG00100 HG00100 3.160473 -4.504760 1.8926507 ## HG00101 HG00101 4.035908 -4.545304 0.9407191 ## HG00102 HG00102 3.608347 -4.668695 0.7327117 To annotate individuals on our plot, we’ll merge pca_results with our metadata table. The merge function combines two tables, merging them by matching a column of your choice (specified with by =). # merge pca_results and metadata pca_results &lt;- merge(pca_results, metadata, # specify columns to merge on by.x = &quot;sample&quot;, by.y = &quot;sample&quot;) head(pca_results) ## sample PC1 PC2 PC3 pop superpop sex ## 1 HG00096 3.060014 -5.822356 -1.2683268 GBR EUR male ## 2 HG00097 2.839200 -6.278675 0.8609691 GBR EUR female ## 3 HG00099 1.803619 -5.171999 0.4033319 GBR EUR female ## 4 HG00100 3.160473 -4.504760 1.8926507 GBR EUR female ## 5 HG00101 4.035908 -4.545304 0.9407191 GBR EUR male ## 6 HG00102 3.608347 -4.668695 0.7327117 GBR EUR female "],["pca-plot.html", "5.14 PCA plot", " 5.14 PCA plot Create a scatterplot of PC1 vs. PC2, coloring by the superpop column. ggplot(data = pca_results, aes(x = PC1, y = PC2, color = superpop)) + geom_point() PC1 separates out the African populations from other populations. This is in line with our knowledge that all non-African populations descend from historical migrations out of Africa – African populations contain significant genetic diversity that is not represented outside of Africa. PC2 seems to separate the East Asian and European populations from the other three superpopulations. Repeat the plot with PC2 vs. PC3. Which superpopulations do you observe separating on PC3? ggplot(data = pca_results, aes(x = PC2, y = PC3, color = superpop)) + geom_point() PC3 separates out the Admixed American and South Asian populations, which were collapsed into one group in the first PCA plot. "],["proportion-of-variance-explained.html", "5.15 Proportion of variance explained", " 5.15 Proportion of variance explained It’s hard to tell from the PCA plot whether the separation of populations we see is meaningful, or if the plot is just exaggerating extremely minor differences between groups. We quantify this by calculating the proportion of variance explained for each PC. This tells us how much of the variation in our data is being captured by PC1, PC2, etc. Variance is the square of the standard deviation, so we can calculate proportion of variance explained from the sdev item in our pca object. Each value corresponds to the standard deviation for one PC. sd &lt;- pca$sdev head(sd) ## [1] 5.692102 3.818282 2.122236 1.954976 1.476041 1.450018 The proportion of variance explained by a PC is its variance, divided by the sum of the variances across all PCs. Conveniently, you can calculate this for every PC at once in R: # divide variance of each PC by sum of all variances var_explained &lt;- sd^2 / sum(sd^2) # proportion of variance explained for: var_explained[1] # PC1 ## [1] 0.09645901 var_explained[2] # PC2 ## [1] 0.04340437 var_explained[3] # PC3 ## [1] 0.01340864 So, PC1 explains only 9.65% of the variance in our data, PC2 explains 4.34%, and PC3 explains 1.34%. Add x and y axis labels to your plots with the proportion of variance explained by each PC. This is common practice for PCA. ggplot(data = pca_results, aes(x = PC1, y = PC2, color = superpop)) + geom_point() + xlab(&quot;PC1 (9.65%)&quot;) + ylab(&quot;PC2 (4.34%)&quot;) ggplot(data = pca_results, aes(x = PC2, y = PC3, color = superpop)) + geom_point() + xlab(&quot;PC2 (4.34%)&quot;) + ylab(&quot;PC3 (1.34%)&quot;) "],["conclusion-3.html", "5.16 Conclusion", " 5.16 Conclusion In this lab, we used genotype data from the 1000 Genomes Project to calculate two measures of population structure. We explored the Geography of Genetic Variants browser, a useful resource for visualizing allele frequency differences between human populations. We introduced VCF format, a common file format for storing genotype data. Using the vcfR package, we read in the VCF and subset to common variants. We plotted the allele frequency spectrum of variants. By calculating \\(\\mathbf{F_{ST}}\\), we found that most variants show very little frequency differentiation across populations. Finally, we used PCA to cluster individuals in our dataset by their genotype information. Plotting individuals in PCA space allowed us to distinguish the five superpopulations of 1000 Genomes. "],["homework-4.html", "5.17 Homework", " 5.17 Homework We’ll now perform PCA using all SNPs in the initial VCF – not just those that were common in 1000 Genomes individuals. In the optional homework, you’ll also use your newly generated PCA plot to predict the ancestry of an unknown sample. 5.17.0.1 Goals &amp; Learning Objectives The goal of this homework is to perform and interpret the results of PCA. Learning Objectives Required homework: Practice performing and interpreting PCA Optional homework: Practice performing PCA and reading code "],["required-homework-3.html", "5.18 Required homework", " 5.18 Required homework Assignment: Re-run the steps we used to generate our PCA plot, this time using the original vcf data object. Do these plots look any different from our plots with just common variants? Solution # extract genotypes and convert to numeric form gt_matrix_all &lt;- extract.gt(vcf) %&gt;% as.data.frame() %&gt;% mutate_all(recode, &quot;0|0&quot; = 0, &quot;1|0&quot; = 1, &quot;0|1&quot; = 1, &quot;1|1&quot; = 2) %&gt;% as.matrix() # transpose gt_matrix_T_all &lt;- t(gt_matrix_all) # perform PCA pca_all &lt;- prcomp(gt_matrix_T_all) # extract coordinates from PCA object x_all &lt;- pca_all$x # create dataframe for plotting pca_results_all &lt;- data.frame(sample = rownames(x_all), PC1 = x_all[, 1], PC2 = x_all[, 2], PC3 = x_all[, 3]) # merge with metadata pca_results_all &lt;- merge(pca_results_all, metadata, # specify columns to merge on by.x = &quot;sample&quot;, by.y = &quot;sample&quot;) # calculate variance explained by each PC var_explained_all &lt;- pca_all$sdev^2 / sum(pca_all$sdev^2) # print for PC1-PC3 var_explained_all[1:3] ## [1] 0.09154081 0.03824824 0.01207284 # PC1 vs. PC2 plot ggplot(data = pca_results_all, aes(x = PC1, y = PC2, color = superpop)) + geom_point() + xlab(&quot;PC1 (9.15%)&quot;) + ylab(&quot;PC2 (3.82%)&quot;) # PC2 vs. PC3 plot ggplot(data = pca_results_all, aes(x = PC2, y = PC3, color = superpop)) + geom_point() + xlab(&quot;PC2 (3.82%)&quot;) + ylab(&quot;PC3 (1.21%)&quot;) The PCA plots actually look pretty similar to the plots with just common variants! "],["optional-homework-1.html", "5.19 Optional homework", " 5.19 Optional homework We can think of our PCA as a model of human individuals. If we have a mystery individual but we know their genotypes for the variants in our PCA, we can predict where they lie in PCA space and thus guess their ancestry. We’ve prepared a VCF, unknown.vcf.gz, with genotypes for one mystery sample (NA21121). The VCF contains the exact same variants as our random_snippet.vcf.gz from this class. Follow the instructions to read in the unknown VCF and predict NA21121’s placement on your PCA plot. To avoid having to re-subset to common variants, we’ll compare NA21121 to our PCA from the required homework (all variants in the VCF). 5.19.0.1 Prepare unknown VCF for PCA Assignment: Using our code from class, read in the unknown VCF file, extract the genotype matrix, recode the genotypes as numbers, and transpose. Solution # read VCF unknown &lt;- read.vcfR(file = &quot;unknown.vcf.gz&quot;) ## Scanning file to determine attributes. ## File attributes: ## meta lines: 19 ## header_line: 20 ## variant count: 10000 ## column count: 10 ## Meta line 19 read in. ## All meta lines processed. ## gt matrix initialized. ## Character matrix gt created. ## Character matrix gt rows: 10000 ## Character matrix gt cols: 10 ## skip: 0 ## nrows: 10000 ## row_num: 0 ## Processed variant 1000 Processed variant 2000 Processed variant 3000 Processed variant 4000 Processed variant 5000 Processed variant 6000 Processed variant 7000 Processed variant 8000 Processed variant 9000 Processed variant 10000 Processed variant: 10000 ## All variants processed # extract and recode genotypes unknown_matrix &lt;- extract.gt(unknown) %&gt;% as.data.frame() %&gt;% mutate_all(recode, &quot;0|0&quot; = 0, &quot;1|0&quot; = 1, &quot;0|1&quot; = 1, &quot;1|1&quot; = 2) %&gt;% as.matrix() # transpose matrix unknown_matrix_T &lt;- t(unknown_matrix) 5.19.0.2 Predict PCA placement of unknown sample Assignment: Run the code below to predict and plot NA21121 on top of your PCA plot from the required homework. If necessary, plot PC2 vs. PC3 as well. What superpopulation do you think NA21121 is from? Solution # predict pca placement of unknown data unknown_pca &lt;- predict(pca_all, unknown_matrix_T) # create dataframe from predicted PCA unknown_results &lt;- data.frame(&quot;PC1&quot; = unknown_pca[, &quot;PC1&quot;], &quot;PC2&quot; = unknown_pca[, &quot;PC2&quot;], &quot;PC3&quot; = unknown_pca[, &quot;PC3&quot;], &quot;sample&quot; = &quot;NA21121&quot;) # plot PC1 vs. PC2 and then predicted sample ggplot() + # PCA plot from required homework geom_point(data = pca_results_all, aes(x = PC1, y = PC2, color = superpop)) + # plots the unknown sample&#39;s location on the PCs geom_label(data = unknown_results, aes(x = PC1, y = PC2, label = sample)) + xlab(&quot;PC1 (9.15%)&quot;) + ylab(&quot;PC2 (3.82%)&quot;) # plot PC2 vs. PC3 ggplot() + geom_point(data = pca_results_all, aes(x = PC2, y = PC3, color = superpop)) + geom_label(data = unknown_results, aes(x = PC2, y = PC3, label = sample)) + xlab(&quot;PC2 (3.82%)&quot;) + ylab(&quot;PC3 (1.21%)&quot;) NA21121 seems to be part of the SAS (South Asian) superpopulation. If we look up the sample ID in the 1000 Genomes database, we can confirm that it’s part of the Gujarati Indians in Houston, TX. "],["genome-wide-association-studies-i.html", "6 Genome-wide association studies I", " 6 Genome-wide association studies I In this lab, we’ll introduce and discuss the limitations of genome-wide association studies (GWAS). 6.0.0.1 Learning objectives After completing this chapter, you’ll be able to: Explain the statistical method that underlies GWAS. Describe the statistical challenges of GWAS. Understand how linkage disequilibrium helps and hinders GWAS. Interpret common GWAS plots and summary statistics. "],["association-studies.html", "6.1 Association studies", " 6.1 Association studies One of the central goals of human genetics is understanding the relationship between genotype and phenotype. Genome-wide association studies (GWAS) emerged ~20 years ago as a useful approach for discovering genetic variation that underlies variation in human traits. To conduct GWAS, you: Go through every variant in the genome Ask if its allele frequency differs between individuals who have or don’t have a phenotype of interest For most variants (Fig. 1, SNP 1), there will be very little difference Identify the variants with the largest association between genotype and phenotype (Fig. 1, SNP 2) Fig. 1. In this GWAS cohort, SNP 2 is significantly associated with the phenotype. The phenotype can be any measurable trait, whether it’s binary (ex: if someone has a disease) or continuous (ex: height). "],["gwas-is-just-linear-regression.html", "6.2 GWAS is just linear regression", " 6.2 GWAS is just linear regression At their core, GWAS involve fitting linear models to test for relationships between variants and phenotypes using data from large samples of individuals. As with the linear models we covered in the DNM module, GWAS fits a line to a set of points. In this case, each point is one individual in the dataset, stratified by their genotype for a variant of interest. Fig. 2 (source). GWAS fits a linear model for every variant, where the x axis is genotype and the y axis is a phenotype. Because there are so many variants in the genome and we perform a separate statistical test for each one, we often end up fitting millions of linear models for a GWAS. "],["multiple-testing.html", "6.3 Multiple testing", " 6.3 Multiple testing What are statistical challenges of performing a test multiple times? When we perform any test multiple times, we increase the risk that a “significant” result is only significant by chance. Under the null hypothesis, we assume that p-values follow a uniform distribution (i.e., a flat distribution from 0 to 1). We can plot this null distribution in R: # generate 1,000,000 &quot;p-values&quot; from a uniform distribution pvalues &lt;- runif(1000000) # histogram with R&#39;s base plotting function hist(pvalues) If we use the typical p-value threshold of \\(0.05\\), 5% of our tests will have \\(p &lt; 0.05\\), even though these p-values were simulated from a null distribution (i.e., no real association). How do we correct for multiple testing? One common multiple testing correction method, Bonferroni correction, sets a stricter p-value threshold. With Bonferroni, you divide your desired p-value by the number of independent tests you conducted. Are GWAS tests (variants) statistically independent? How does this affect our p-value threshold? As we learned in the LD module, the genotypes of nearby variants are correlated. This non-independence means that we can be less strict with multiple testing correction, because we aren’t performing as many independent tests as we think we are. Researchers have calculated that \\(\\mathbf{5*10^{-8}}\\) is an appropriate p-value threshold for GWAS in humans, given the amount of LD in human genomes. "],["ld-and-gwas.html", "6.4 LD and GWAS", " 6.4 LD and GWAS LD is both a blessing and a curse for GWAS. On one hand, LD means that we need not genotype every SNP to discover associations. We merely need to genotype “tag SNPs”, which are in LD with variants that causally influence the phenotype. On the other hand, this also means that even when we find a signficant association, it is often challenging to disentangle the causal gene and/or variant that drives the association from all the other variants on the same haplotype. Fig. 3 (source). Even without finding the causal variant, we can still discover the causal haplotype through genotyping variants in LD . "],["imputation.html", "6.5 Imputation", " 6.5 Imputation LD also means that we can perform imputation to improve our GWAS discovery power. If we know which variants tend to occur together on the same haplotype, we can infer the presence of variants in an individual even if those SNPs were never sequenced. In imputation, you: Genotype individuals using the sequencing data you have (Fig. 4A) Use a reference panel of haplotypes to fill in variants you didn’t genotype (Fig. 4B) Perform GWAS Imputation can discover GWAS signals that were hidden in the original variant set (Fig. 4B vs. Fig. 4F), or further support signals you already found. Fig. 4 (source). Imputing variants that were not actually sequenced. "],["qq-plots.html", "6.6 QQ plots", " 6.6 QQ plots One common visualization for GWAS results is a QQ plot, which compares the distribution of p-values in our results to a null distribution (i.e., the uniform distribution that we plotted earlier). How do you make a QQ plot? Generate simulated p-values from a uniform distribution – the number of simulated p-values should equal the number of actual p-values Sort both your real and simulated p-values in descending order Plot the first, second, third, etc. p-values, where x-axis is the simulated value y-axis is the actual value Fig. 5 (source). QQ plots visualize the distriution of p-values compared to a null distribution. There are three areas of this plot where points can fall: On the \\(\\mathbf{x = y}\\) line: No association signal Above the \\(\\mathbf{x = y}\\) line: Some association signal Below the \\(\\mathbf{x = y}\\) line: Issue with our statistical test (ex: not appropriately adjusting for covariates) "],["manhattan-plots.html", "6.7 Manhattan plots", " 6.7 Manhattan plots Manhattan plots show the distribution of GWAS hits across the genome, where the y-axis is p-value. &lt;/br. Fig. 6. Manhattan plot of GWAS results. The red line is the \\(5*10^{-8}\\) genome-wide significance threshold. Why are there peaks in the Manhattan plot? Each peak is composed of variants from the same haplotype, which all have a strong association with the phenotype because of LD. "],["sample-size.html", "6.8 Sample size", " 6.8 Sample size As GWAS sample size has increased over the years, each larger study is able to discover more variants. Fig. 7 (source). Increase in GWAS discovery power with sample size. Why do we find more associations with larger studies? A larger dataset captures more rare variation A larger dataset provides more statistical power – we can more confidently say whether allele frequencies are different between individuals with and without a phenotype "],["interpreting-gwas-results.html", "6.9 Interpreting GWAS results", " 6.9 Interpreting GWAS results Let’s look at a browser of GWAS results, generated by a study that used genotype and phenotype data from the UK Biobank. UK Biobank UK Biobank is one of the largest cohorts of genotype and phenotype data available. This study has data from ~500,000 individuals across the UK, including health records and responses to surveys, all of which are publicly available for research. As an example, let’s look up associations with standing height. Fig. 8. GWAS results for standing height. For each phenotype, the browser shows a Manhattan plot of associated variants The top p-values for this trait look extremely significant Bottom of the page also has a QQ plot Although these p-values are huge, keep in mind that for the vast majority of extremely strong associations, the total proportion of variance explained is very small (&lt; 10%). (i.e., most of the natural variation in the phenotype is not explained by that variant.) "],["conclusion-4.html", "6.10 Conclusion", " 6.10 Conclusion In this lab, we gave a conceptual overview of genome-wide association studies. GWAS are an attempt to answer one of the oldest questions in genetics: How does genotype impact phenotype? In GWAS, you go through every variant in the genome and fit a linear model to ask if genotype at that variant is associated with a phenotype of interest. Multiple testing correction accounts for the statistical burden of these tests. Linkage disequilibrium helps us discover more GWAS hits, but also limits our ability to identify causal variants/genes. QQ plots and Manhattan plots are common visualizations of GWAS results. "],["homework-5.html", "6.11 Homework", " 6.11 Homework 6.11.0.1 Learning Objectives Interpret the summary statistics typically reported in GWAS studies Understand the limitations and biases of GWAS 6.11.0.2 Assignment Find any GWAS paper. Read it and report: Phenotype being studied Sample size Population being studied (homogeneous? Multi-ethnic? If it’s multi-ethnic, how do they correct for the effect of ancestry?) For the top asssociation: p-value (would you call it significant genome wide?) Effect size Did the authors replicate the result in an independent cohort? Haplotype structure, nearby genes, causal variant Example solution Example GWAS: Genome-wide analysis identifies genetic effects on reproductive success and ongoing natural selection at the FADS locus. Phenotype: Number of children ever born Sample size:: 785,604 Population: European; no population structure correction (but did control for family structure) Top asssociation: rs201815280, chr3:85546181, A-&gt;ACACC (from Supp. Table 7) p-value: 5.25e-26, seems genome-wide significant Effect size: 0.0249 (with every copy of the ACACC allele, individuals have one more child born) Replication: Did not replicate in a cohort of 34,367 women from the FinnGen study (p = 0.177) Haplotype structure, nearby genes, causal variant: In an intron of CADM2; no causal variant or LD analysis "],["genome-wide-association-studies-ii.html", "7 Genome-wide association studies II", " 7 Genome-wide association studies II In this lab, we’ll perform a GWAS with data adapted from a [workshop created by Heather Wheeler]((https://github.com/hwheeler01/GWAS_workshop) from Loyola University Chicago. 7.0.0.1 Learning objectives After completing this chapter, you’ll be able to: Perform GWAS of a single SNP using linear regression. Use PLINK to perform GWAS on all SNPs in a VCF. Create and interpret common GWAS visualization plots. "],["setup-3.html", "7.1 Setup", " 7.1 Setup The premise for this exercise is that you’re part of a company developing a cancer drug called GS451. Some individuals in early phase trials have experienced a side effect of the drug called lymphocytopenia (low lymphocyte counts). You are now tasked with performing a GWAS on data from lymphblastoid cell lines to search for risk factors for lymphocytopenia. The phenotype that was measured is the \\(\\mathbf{IC_{50}}\\), defined as the concentration of the drug at which the cells experience 50% viability. 7.1.1 R packages We’ll use the tidyverse and vcfR packages we’ve seen in previous weeks, as well as qqman, which includes functions for creating Manhattan and QQ plots. library(tidyverse) library(vcfR) library(qqman) 7.1.2 Data GWAS requires information on both the genotype and a phenotype of interest in the same individuals. The genotype data we’re using are real data from the Yoruba population in the 1000 Genomes Project, but the phenotype data is simulated. Why can’t we use real phenotype data? The combination of genotype and phenotype data poses a privacy risk, so real genotype and phenotype data are often stored in controlled-access databases such as dbGaP. Although these data are still available to researchers who want to work with it, access usually requires submitting an application to explain what your intend to do with it. "],["genotype-data-1.html", "7.2 Genotype data", " 7.2 Genotype data As in the population structure module, our genotype data is stored in a Variant Call Format (VCF) file. The genotypes_subset.vcf file is a small subset of a much larger VCF that we’ll use later to run a genome-wide GWAS. # load the VCF vcf &lt;- read.vcfR(&quot;genotypes_subset.vcf&quot;) ## Scanning file to determine attributes. ## File attributes: ## meta lines: 27 ## header_line: 28 ## variant count: 72 ## column count: 185 ## Meta line 27 read in. ## All meta lines processed. ## gt matrix initialized. ## Character matrix gt created. ## Character matrix gt rows: 72 ## Character matrix gt cols: 185 ## skip: 0 ## nrows: 72 ## row_num: 0 ## Processed variant: 72 ## All variants processed We’ll first extract just the first SNP in the dataset, using the vcfR2tidy function to convert the vcf object into tidy dataframes. # extract first SNP and convert to tidy df test_snp &lt;- vcfR2tidy(vcf[1, ]) ## Extracting gt element GT The gt dataframe within test_snp contains the genotypes of our SNP of interest. In this dataframe, every row is a different individual in the dataset. # extract genotype dataframe test_snp_gt &lt;- test_snp$gt head(test_snp_gt) ## # A tibble: 6 × 5 ## ChromKey POS Indiv gt_GT gt_GT_alleles ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 558185 1001_1001 0/0 A/A ## 2 1 558185 1002_1002 0/0 A/A ## 3 1 558185 1003_1003 0/0 A/A ## 4 1 558185 1004_1004 0/1 A/G ## 5 1 558185 1005_1005 0/0 A/A ## 6 1 558185 1006_1006 0/1 A/G "],["counting-allele-dosage.html", "7.3 Counting allele dosage", " 7.3 Counting allele dosage We’re often interested in encoding genotypes as a 0, 1, or 2, which you can think of as the “dosage” of the minor allele. This is an additive model, and assumes that the phenotype of the heterozygote is intermediate between those of the two homozygotes. We can use the table function on the gt_GT_alleles column to quickly check how many individuals have each genotype. # tabulate genotype counts table(test_snp_gt$gt_GT_alleles) ## ## A/A A/G ## 66 20 Now we’ll use the mutate function to create a new column of the dataframe that counts the dosage of the minor allele (i.e., how many G’s each person has at that SNP): # convert genotypes to counts (i.e., dosage) of minor allele test_snp_gt &lt;- test_snp_gt %&gt;% # count number of Gs mutate(dosage = str_count(gt_GT_alleles, &quot;G&quot;)) %&gt;% drop_na() head(test_snp_gt) ## # A tibble: 6 × 6 ## ChromKey POS Indiv gt_GT gt_GT_alleles dosage ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 1 558185 1001_1001 0/0 A/A 0 ## 2 1 558185 1002_1002 0/0 A/A 0 ## 3 1 558185 1003_1003 0/0 A/A 0 ## 4 1 558185 1004_1004 0/1 A/G 1 ## 5 1 558185 1005_1005 0/0 A/A 0 ## 6 1 558185 1006_1006 0/1 A/G 1 Checking our work with table If we run table on the dosage column, we should get the same breakdown of genotypes as we got from the gt_GT_alleles columns. # make sure we get the same genotype counts table(test_snp_gt$dosage) ## ## 0 1 ## 66 20 "],["phenotype-data.html", "7.4 Phenotype data", " 7.4 Phenotype data Our phenotype for this GWAS is the \\(\\mathbf{IC_{50}}\\) – the concentration of the GS451 drug that at which we observe 50% viability in cell culture. # read in phenotypes phenotypes &lt;- read.table(&quot;GS451_IC50.txt&quot;, header = TRUE) head(phenotypes) ## FID IID GS451_IC50 ## 1 1001 1001 5.594256 ## 2 1002 1002 8.525633 ## 3 1003 1003 12.736739 ## 4 1004 1004 12.175201 ## 5 1005 1005 9.936742 ## 6 1006 1006 9.163483 The columns of this table are: FID &amp; IID: Family and individual IDs of the individual GS451_IC50: Measured \\(\\mathrm{IC_{50}}\\) for the drug of interest Plot the distribution of the phenotype. ggplot(data = phenotypes, aes(x = GS451_IC50)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 1 rows containing non-finite values (stat_bin). This data looks approximately normally distributed. This is important to check because this is one of the assumptions of linear regression, which we’ll be using to perform the GWAS. "],["merging-genotype-and-phenotype-data.html", "7.5 Merging genotype and phenotype data", " 7.5 Merging genotype and phenotype data To perform a GWAS, we need to combine genotype and phenotype information for the same individuals. This means merging these two data for our SNP of interest. Merging requires the sample ID to be identical between the two dataframes. Note that in test_snp_gt, the sample ID is in the format FID_IID (i.e., 1001_1001), whereas in phenotypes the FID and IID are separate. In order to merge, we create a new column in phenotypes that combines them: phenotypes &lt;- phenotypes %&gt;% # create column called `Indiv` that combines FID and IID mutate(Indiv = paste(FID, IID, sep = &quot;_&quot;)) head(phenotypes) ## FID IID GS451_IC50 Indiv ## 1 1001 1001 5.594256 1001_1001 ## 2 1002 1002 8.525633 1002_1002 ## 3 1003 1003 12.736739 1003_1003 ## 4 1004 1004 12.175201 1004_1004 ## 5 1005 1005 9.936742 1005_1005 ## 6 1006 1006 9.163483 1006_1006 Now we can use merge to join the data: # merge genotype and phenotype info for test SNP gwas_data &lt;- merge(test_snp_gt, phenotypes, by = &quot;Indiv&quot;) head(gwas_data) ## Indiv ChromKey POS gt_GT gt_GT_alleles dosage FID IID GS451_IC50 ## 1 1001_1001 1 558185 0/0 A/A 0 1001 1001 5.594256 ## 2 1002_1002 1 558185 0/0 A/A 0 1002 1002 8.525633 ## 3 1003_1003 1 558185 0/0 A/A 0 1003 1003 12.736739 ## 4 1004_1004 1 558185 0/1 A/G 1 1004 1004 12.175201 ## 5 1005_1005 1 558185 0/0 A/A 0 1005 1005 9.936742 ## 6 1006_1006 1 558185 0/1 A/G 1 1006 1006 9.163483 "],["gwas-for-one-variant.html", "7.6 GWAS for one variant", " 7.6 GWAS for one variant Under the hood, GWAS is just linear regression – simple statistical models to assess evidence of a relationship between two variables. We can perform this linear regression by hand, using data from the first SNP in the VCF. In our model, we’ll be asking whether there’s a relationship between an individual’s genotype (their dosage of the SNP) and phenotype (their \\(\\mathrm{IC_{50}}\\) for GS451). Why did we merge our genotype and phenotype data? When we fit linear models in the DNM module, we needed our variables (age and # of DNMs) to be separate columns of the same table. Similarly, now that our variables are genotype and phenotype, they need to be in the same dataframe. "],["genotype-phenotype-boxplots.html", "7.7 Genotype-phenotype boxplots", " 7.7 Genotype-phenotype boxplots First, let’s plot the relationship between genotype and phenotype to see if it looks interesting. Create boxplots of the phenotype, stratified by genotype of the test SNP. ggplot(data = gwas_data) + geom_boxplot(aes(x = gt_GT_alleles, y = GS451_IC50)) ## Warning: Removed 1 rows containing non-finite values (stat_boxplot). It’s unclear whether there’s a relationship here, because the phenotype distributions for these two genotypes are mostly overlapping. To be certain, we’ll now test this with linear regression. "],["linear-regression.html", "7.8 Linear regression", " 7.8 Linear regression The function to perform linear regression in R is lm(). It takes as arguments a data frame (gwas_data) and a model formula of the form outcome ~ predictors. In the case of GWAS, our outcome is the phenotype, and our predictor is the SNP genotype. We may also include covariates such as sex, age, or ancestry as additional predictors (called covariates) to control for their potential confounding effects. No such data are available here, so we just run the simple genotype vs. phenotype test. # test for association between genotype and phenotype lm(data = gwas_data, formula = GS451_IC50 ~ dosage) %&gt;% # directly pipe (%&gt;%) model results to the `summary()` function summary() ## ## Call: ## lm(formula = GS451_IC50 ~ dosage, data = gwas_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.0247 -1.9643 -0.3867 2.1967 6.6201 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.5712 0.3299 19.921 &lt;2e-16 *** ## dosage 1.3846 0.6800 2.036 0.0449 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.659 on 83 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.04757, Adjusted R-squared: 0.0361 ## F-statistic: 4.146 on 1 and 83 DF, p-value: 0.04493 How do we interpret the results of the linear model? The coefficient for dosage indicates that on average, each copy of the “G” allele increases \\(\\mathrm{IC_{50}}\\) by \\(1.38\\). The p-value indicates that this slope of \\(1.38\\) is significantly greater than 0 (\\(p = 0.0449\\)). Do you think this SNP would reach genome-wide significance? This p-value is borderline, sitting very close to the arbitrary cutoff of \\(0.05\\) which is generally used to determine statistical significance. If this was the only SNP that we were investigating, we might find this result promising. However, this SNP is just one of hundreds of thousands of SNPs that we will test for association, so the burden of proof will need to be much higher. Recall that the genome-wide significance threshold for GWAS in humans is \\(5 * 10^{-8}\\). "],["gwas-for-multiple-snps.html", "7.9 GWAS for multiple SNPs", " 7.9 GWAS for multiple SNPs A GWAS performs the linear regression we just did, for every SNP in the dataset. We could write a for loop to do this in R ourselves, but it would be a bit slow because there are 256,896 variants in the full VCF. Because GWAS is such a common approach, researchers have developed software to standardize this process and make it extremely efficient. The most popular software package for GWAS is called PLINK, which is preloaded into your Cloud session. PLINK is a “command-line” tool, so we could either use it by working from the “Terminal” tab in Posit Cloud, or using the system() command within R. For this class we’ll use the latter approach. What’s the system() command? The command line is a text interface that takes in commands for your computer’s operating system to run. RStudio and Posit Cloud are a more interactive interface for writing code that you’d normally have to run on the command line. The system() command tells RStudio to run a snippet of command line code for you, without you having to leave the R environment. "],["gwas-of-one-snp-with-plink.html", "7.10 GWAS of one SNP with PLINK", " 7.10 GWAS of one SNP with PLINK First, we’ll replicate the GWAS that we did in R with just the first SNP of the VCF, rs9699599. # replicate first SNP association with PLINK system(command = &quot;./plink --file genotypes --linear --snp rs9699599 --pheno GS451_IC50.txt --pheno-name GS451_IC50&quot;) Breakdown of the PLINK command ./plink: Use the PLINK software --file genotypes: Genotype data files (genotypes.map, genotypes.ped) begin with the string “genotypes” --linear:Run a linear additive association test for each SNP --snp rs9699599: Only run the analysis for a single SNP (rs9699599) --pheno GS451_IC50.txt: Phenotype data is located in a file called GS451_IC50.txt --pheno-name GS451_IC50: The column heading of the phenotype to use in the phenotype file is GS451_IC50 After running PLINK, we get an output file called plink.assoc.linear. Now look at the output of the plink.assoc.linear output file that PLINK produced. snp1 &lt;- read.table(&quot;plink.assoc.linear&quot;, header = TRUE) head(snp1) ## CHR SNP BP A1 TEST NMISS BETA STAT P ## 1 1 rs9699599 558185 G ADD 85 1.385 2.036 0.04493 How do these results compare to performing the GWAS by hand? Notice that the beta (i.e., the “slope” or coefficient) and p-value perfectly matches the results we obtained previously with R. "],["gwas-of-all-snps-with-plink.html", "7.11 GWAS of all SNPs with PLINK", " 7.11 GWAS of all SNPs with PLINK Now let’s allow PLINK to run the statistical tests for all SNPs by removing the --snp flag. system(command = &quot;./plink --file genotypes --linear --pheno GS451_IC50.txt --pheno-name GS451_IC50&quot;) The plink.assoc.linear file should now have ~260,000 lines. Load the file into R to look at the results: results &lt;- read.table(file = &quot;plink.assoc.linear&quot;, header = TRUE) %&gt;% # order table by lowest pvalue arrange(P) head(results) ## CHR SNP BP A1 TEST NMISS BETA STAT P ## 1 19 rs7257475 20372113 T ADD 88 -3.008 -6.876 9.311e-10 ## 2 19 rs10413538 20370690 T ADD 86 -3.026 -6.805 1.395e-09 ## 3 21 rs2826383 20844081 A ADD 166 3.031 5.866 2.392e-08 ## 4 19 rs12972967 20358400 T ADD 89 -2.421 -5.939 5.760e-08 ## 5 2 rs1358578 51626897 A ADD 166 2.111 5.307 3.571e-07 ## 6 17 rs3094508 33137048 C ADD 89 3.532 5.230 1.156e-06 "],["plotting-gwas-results.html", "7.12 Plotting GWAS results", " 7.12 Plotting GWAS results The qq() and manhattan() functions in the qqman package let us easily create QQ and Manhattan plots to visualize our GWAS results. # qq plot using the P (pvalues) column qq(results$P) # manhattan plot manhattan(results) SNPs with low p-values occur in peaks of multiple variants. These are not independent associations, but rather groups of variants in LD. "],["top-gwas-snp.html", "7.13 Top GWAS SNP", " 7.13 Top GWAS SNP One common future direction for GWAS studies is following up on the top SNP(s). Read in top_snp.vcf, a VCF of just the top SNP in the dataset, so that we can plot boxplots of the top SNP genotype stratified by phenotype: # extract genotypes of the top SNP top_snp &lt;- vcfR2tidy(read.vcfR(&quot;top_snp.vcf&quot;)) ## Scanning file to determine attributes. ## File attributes: ## meta lines: 27 ## header_line: 28 ## variant count: 1 ## column count: 185 ## Meta line 27 read in. ## All meta lines processed. ## gt matrix initialized. ## Character matrix gt created. ## Character matrix gt rows: 1 ## Character matrix gt cols: 185 ## skip: 0 ## nrows: 1 ## row_num: 0 ## Processed variant: 1 ## All variants processed ## Extracting gt element GT top_snp_gt &lt;- top_snp$gt %&gt;% drop_na() # merge with phenotype data gwas_data &lt;- merge(top_snp_gt, phenotypes, by = &quot;Indiv&quot;) # plot boxplots ggplot(data = gwas_data) + geom_boxplot(aes(x = gt_GT_alleles, y = GS451_IC50)) ## Warning: Removed 1 rows containing non-finite values (stat_boxplot). Other potential follow-up directions include: Investigating the genomic environment in the UCSC Genome Browser Looking at nearby haplotype structure with LDproxy Note that the genotype data we’re using come from the Yoruba population Using the Geography of Genetic Variants browser to find the global allele frequencies of the variant Search for SNP in a phenotype database to see if there are other associations with it "],["conclusion-5.html", "7.14 Conclusion", " 7.14 Conclusion We used genotype and simulated phenotype data from the 1000 Genomes Project to perform a genome-wide association study for variants associated with drug \\(\\mathrm{IC_{50}}\\). Using linear regression, we first did GWAS “by hand” on just one variant in the VCF. We fit a linear model to ask whether there’s a significant relationship between genotype and phenotype. We then used PLINK to perform this test on every SNP in the genome. We followed up on the top SNP from our GWAS by plotting boxplots of phenotype stratified by genotype. "],["homework-6.html", "7.15 Homework", " 7.15 Homework 7.15.0.1 Learning Objectives Interpret results of a GWAS Practice manipulating vcfR and tabular data 7.15.0.2 Assignment Run a GWAS of \\(\\mathrm{IC_{50}}\\) for the drug CB1908, using the same genotype data as before. The phenotypes are located in CB1908_IC50.txt. Make a QQ plot and a Manhattan plot of your results. Do you have any genome-wide significant hits? Are they located in or near a gene? For the top GWAS hit, plot the phenotype stratified by genotype. (Use top_snp_hw.vcf to get the genotypes of the top hit.) Solution # perform association test with PLINK system(command = &quot;./plink --file genotypes --linear --pheno CB1908_IC50.txt --pheno-name CB1908_IC50&quot;) # read in gwas results results &lt;- read.table(file = &quot;plink.assoc.linear&quot;, header = TRUE) %&gt;% mutate(index = row_number()) %&gt;% arrange(P) # qq plot qq(results$P) # manhattan plot manhattan(results) On the Manhattan plot, there’s one hit that reaches genome-wide significance. # view top GWAS hit results[1, ] ## CHR SNP BP A1 TEST NMISS BETA STAT P ## 1 12 rs10876043 49190411 G ADD 161 1.779 7.18 2.518e-11 From looking it up in the UCSC Genome Browser, rs10876043 lies within an intron of the DIP2B gene. Finally, we plot this SNP’s genotype stratified by phenotype. (We can extract its genotype information from the vcf object because we know its index number from the results table.) # extract top SNP and convert to tidy df top_snp &lt;- vcfR2tidy(read.vcfR(&quot;top_snp_hw.vcf&quot;)) ## Scanning file to determine attributes. ## File attributes: ## meta lines: 27 ## header_line: 28 ## variant count: 1 ## column count: 185 ## Meta line 27 read in. ## All meta lines processed. ## gt matrix initialized. ## Character matrix gt created. ## Character matrix gt rows: 1 ## Character matrix gt cols: 185 ## skip: 0 ## nrows: 1 ## row_num: 0 ## Processed variant: 1 ## All variants processed ## Extracting gt element GT # get genotype dataframe top_snp_gt &lt;- top_snp$gt %&gt;% drop_na() # read in phenotype dataframe phenotypes &lt;- read.table(&quot;CB1908_IC50.txt&quot;, header = TRUE) %&gt;% # create column called `Indiv` that combines FID and IID mutate(Indiv = paste(FID, IID, sep = &quot;_&quot;)) # merge genotype and phenotype info gwas_data &lt;- merge(top_snp_gt, phenotypes, by = &quot;Indiv&quot;) # plot genotype by phenotype boxplots ggplot(data = gwas_data) + geom_boxplot(aes(x = gt_GT_alleles, y = CB1908_IC50)) ## Warning: Removed 2 rows containing non-finite values (stat_boxplot). "],["scans-for-selection.html", "8 Scans for selection", " 8 Scans for selection In this lab, we’ll explore three methods for identifying signatures of selection: FST, population branch statistic (PBS), and extended haplotype homozygosity (EHH). 8.0.0.1 Learning objectives After completing this chapter, you’ll be able to: Describe the genetic signatures of selection and what they reveal about the strength and timing of the selective sweep. Calculate and interpret population differentiation with FST. Understand why PBS provides additional information over two-population tests of selection. Explain how EHH and iHS leverage haplotype information to identify selection. "],["signatures-of-positive-selection.html", "8.1 Signatures of positive selection", " 8.1 Signatures of positive selection A central question in human evolutionary genetics is what genetic adaptations humans acquired as they migrated into diverse environments. Fig. 1 (source). The history of human migrations. The process of evolutionary adaptation leaves characteristic signatures on patterns of population genetic variation. By developing statistics to capture these signatures and applying them to human genetic data, we can uncover evidence of past episodes of positive selection. Statistics for identifying signatures of selection can be broadly classified into two categories: frequency-based and haplotype-based. "],["frequency-based-signatures.html", "8.2 Frequency-based signatures", " 8.2 Frequency-based signatures One signature of selection is an allele whose frequency is either smaller or larger than you would expect. Fig. 2. The allele frequencies of this variant shows large differences between population. How do we determine the expected AF? We can compare AFs between populations. On average, neutrally evolving variants should have similar frequencies across populations. We can enrich for potential targets of selection by asking which variants show the biggest population-specific frequency differences. If a variant shows large AF differences and the populations in question share common ancestry – as all populations do – then the allele frequency must have changed in one population. We can also supplement this with knowledge about when the populations diverged to determine when and how quickly this AF change must have happened. "],["haplotype-based-signatures.html", "8.3 Haplotype-based signatures", " 8.3 Haplotype-based signatures When a variant changes in frequency, it doesn’t change alone. The variants on the haplotype surrounding it will be pulled along to high frequency. This phenomenon, called hitchhiking, is similar to the haplotype blocks we observe in GWAS result (and both are caused by linkage between variants). What do you expect to happen to genetic diversity in the region of a selective sweep? If one haplotype is sweeping to high AF, we expect that genetic diversity in the region decreases because it’s being replaced by the selected haplotype. This is called a selective sweep because variation is being “swept” out of the region. This decreased diversity signature fades over time as new mutations arise on the haplotype. Fig. 3. (A) Selective sweeps reduce genetic diversity. (B)-(D) Summary of common signatures of selection. What can the size of the linked haplotype tell us about its evolutionary history? Because recombination breaks down haplotypes over time, a longer haplotype implies more recent selection. It can also provide information on the strength of selection – if most occurrences of the haplotype in the population are unbroken, selection was strong enough that it didn’t have time to recombine during the sweep. "],["setup-4.html", "8.4 Setup", " 8.4 Setup 8.4.1 R packages In addition to tidyverse and vcfR, we’ll also be using ggtree, an R package for visualizing trees associated with PBS outliers. library(tidyverse) library(vcfR) library(ggtree) "],["the-fst-statistic.html", "8.5 The FST statistic", " 8.5 The FST statistic \\(\\mathbf{F_{ST}}\\) is a statistic that quantifies differences in allele frequencies between populations at one variable site. The version of \\(\\mathrm{F_{ST}}\\) that we’ll calculate today compares genotypic variance within subpopulations (“S”) against the total population (“T”). One way to conceptualize this is the deficiency of heterozygotes observed across subpopulations, relative to the proportion that would be expected under random mating (i.e., no population structure). We calculate this by taking the difference between: \\(\\mathbf{H_T}\\): The expected frequency of heterozygotes when individuals across all subpopulations are pooled \\(\\mathbf{mean(H_S)}\\): The mean frequency of heterozygotes, calculated within each subpopulation and then averaged where \\(H = 2pq\\), and \\(p\\) and \\(q\\) are the frequencies of the two alleles at a site \\[ \\textrm{F}_{ST} = \\frac{H_T - \\textrm{mean}(H_S)}{H_T} \\] \\(\\mathrm{F_{ST}}\\) ranges from 0 to 1: \\(\\mathrm{F_{ST}} = 0\\): No population structure (separating the subpopulations doesn’t affect heterozygosity estimates) \\(\\mathrm{F_{ST}} = 1\\): Subopulations are very different (ex: one population only carries one allele, while the other population only carries the other) See this Nature Review Genetics article for a more thorough discussion on the use and interpretation of \\(\\mathrm{F_{ST}}\\) and related statistics. "],["data-for-fst.html", "8.6 Data (for FST)", " 8.6 Data (for FST) We’re once again working with data from the 1000 Genomes Project. As in the population structure module, we’ll read in genotype data (in VCF form) and metadata on the populations that each sample is from. # read genotype data with vcfR vcf &lt;- read.vcfR(file = &quot;random_variable_sites.vcf.gz&quot;) ## Scanning file to determine attributes. ## File attributes: ## meta lines: 19 ## header_line: 20 ## variant count: 9748 ## column count: 2513 ## Meta line 19 read in. ## All meta lines processed. ## gt matrix initialized. ## Character matrix gt created. ## Character matrix gt rows: 9748 ## Character matrix gt cols: 2513 ## skip: 0 ## nrows: 9748 ## row_num: 0 ## Processed variant 1000 Processed variant 2000 Processed variant 3000 Processed variant 4000 Processed variant 5000 Processed variant 6000 Processed variant 7000 Processed variant 8000 Processed variant 9000 Processed variant: 9748 ## All variants processed # read metadata metadata &lt;- read.table(&quot;integrated_call_samples.txt&quot;, header = TRUE) head(metadata) ## sample pop superpop sex ## 1 HG00096 GBR EUR male ## 2 HG00097 GBR EUR female ## 3 HG00099 GBR EUR female ## 4 HG00100 GBR EUR female ## 5 HG00101 GBR EUR male ## 6 HG00102 GBR EUR female "],["the-genetic_diff-function.html", "8.7 The genetic_diff function", " 8.7 The genetic_diff function We’ll compute \\(\\textrm{F}_{ST}\\) using vcfR’s genetic_diff function. (This function technically calculates \\(\\textrm{G}_{ST}\\), a version of \\(\\textrm{F}_{ST}\\) that considers when there are more than two alleles at a given locus. When a locus is biallelic, \\(\\textrm{F}_{ST} = \\textrm{G}_{ST}\\).) ?genetic_diff genetic_diff requires: vcfR object (in our case, vcf) Factor indicating populations What’s “Factor indicating populations”? The second object for genetic_diff needs to be a vector (i.e., a list) of population labels for the samples in the VCF. These labels must be factors, which is an R data type that limits a variable to a set of values. In our case, these values are the specific population labels in our dataset. We’ll be using the superpopulation groupings for this calculation. We can use our metadata table to generate a vector of superpopulation labels. Since the superpopulation IDs are in the superpop column of that dataframe, we can convert the column from character to factor values with the as.factor function. pop_labels &lt;- as.factor(metadata$superpop) head(pop_labels) ## [1] EUR EUR EUR EUR EUR EUR ## Levels: AFR AMR EAS EUR SAS Previewing pop_labels shows us that there are five “levels” in this vector, where each level is a superpopulation name. "],["calculating-fst.html", "8.8 Calculating FST", " 8.8 Calculating FST Run genetic_diff on the VCF: # calculate gst gst_results &lt;- genetic_diff(vcf, pop_labels) %&gt;% # order dataframe by descending gst value arrange(-Gst) # preview highest gst variants head(gst_results) ## CHROM POS Hs_AFR Hs_AMR Hs_EAS Hs_EUR Hs_SAS ## 1 chr21 17753762 0.3537087 0.20174987 0.1326531 0.029319019 0.040063399 ## 2 chr21 18668817 0.1477089 0.38831400 0.1377374 0.424382716 0.414679179 ## 3 chr21 15620159 0.4997750 0.09318240 0.0000000 0.007905014 0.000000000 ## 4 chr21 16235733 0.4994938 0.09318240 0.0000000 0.007905014 0.002042899 ## 5 chr21 22780904 0.4992826 0.09836474 0.0000000 0.001982159 0.000000000 ## 6 chr21 22786927 0.4991001 0.09318240 0.0000000 0.001982159 0.026231489 ## Ht n_AFR n_AMR n_EAS n_EUR n_SAS Gst Htmax Gstmax ## 1 0.3650242 1320 694 1008 1008 978 0.5572530 0.8286973 0.8049790 ## 2 0.4847713 1320 694 1008 1008 978 0.4082388 0.8484665 0.6618973 ## 3 0.2439190 1320 694 1008 1008 978 0.4004813 0.8289905 0.8235999 ## 4 0.2341095 1320 694 1008 1008 978 0.3739730 0.8290489 0.8232205 ## 5 0.2323592 1320 694 1008 1008 978 0.3732539 0.8288159 0.8242912 ## 6 0.2346916 1320 694 1008 1008 978 0.3609202 0.8297041 0.8192288 ## Gprimest ## 1 0.6922578 ## 2 0.6167706 ## 3 0.4862571 ## 4 0.4542805 ## 5 0.4528180 ## 6 0.4405610 # preview lowest gst variants tail(gst_results) ## CHROM POS Hs_AFR Hs_AMR Hs_EAS Hs_EUR Hs_SAS ## 9743 chr21 45527242 0.001514004 0.000000000 0.000000000 0.001982159 0.002042899 ## 9744 chr21 46135735 0.001514004 0.000000000 0.001982159 0.000000000 0.002042899 ## 9745 chr21 10718788 0.001514004 0.000000000 0.001982159 0.001982159 0.000000000 ## 9746 chr21 43949497 0.001514004 0.002877692 0.001982159 0.000000000 0.002042899 ## 9747 chr21 33087300 0.003025712 0.005747079 0.005934666 0.003960380 0.004081616 ## 9748 chr21 7948042 0.499885216 0.499995847 0.499968506 0.499992126 0.500000000 ## Ht n_AFR n_AMR n_EAS n_EUR n_SAS Gst Htmax Gstmax ## 9743 0.001197365 1320 694 1008 1008 978 3.251980e-04 0.7924231 0.9984895 ## 9744 0.001197365 1320 694 1008 1008 978 3.251980e-04 0.7924231 0.9984895 ## 9745 0.001197365 1320 694 1008 1008 978 3.150481e-04 0.7924255 0.9984895 ## 9746 0.001596168 1320 694 1008 1008 978 2.547455e-04 0.7924784 0.9979864 ## 9747 0.004383322 1320 694 1008 1008 978 1.475473e-04 0.7930368 0.9944736 ## 9748 0.499976954 1320 694 1008 1008 978 3.141686e-05 0.8960702 0.4420513 ## Gprimest ## 9743 3.256899e-04 ## 9744 3.256899e-04 ## 9745 3.155247e-04 ## 9746 2.552595e-04 ## 9747 1.483672e-04 ## 9748 7.107064e-05 genetic_diff outputs a table of \\(\\textrm{G}_{ST}\\) results, where every line corresponds to one variant from the input VCF. Our \\(\\textrm{G}_{ST}\\) values range from \\(0.0021\\) to \\(0.00033\\). "],["distribution-of-gst-across-the-genome.html", "8.9 Distribution of GST across the genome", " 8.9 Distribution of GST across the genome Plot the distribution of \\(\\textrm{G}_{ST}\\) values from genetic_diff ggplot(data = gst_results, aes(x = Gst)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. This is an exponentially decaying distribution, indicating that most common human variants don’t show strong differences in allele frequency between populations. "],["top-gst-hits.html", "8.10 Top GST hits", " 8.10 Top GST hits The variants with high \\(\\textrm{G}_{ST}\\) values should show differences in allele frequency between populations. Let’s examine the top-scoring variant: gst_results[1, ] ## CHROM POS Hs_AFR Hs_AMR Hs_EAS Hs_EUR Hs_SAS Ht ## 1 chr21 17753762 0.3537087 0.2017499 0.1326531 0.02931902 0.0400634 0.3650242 ## n_AFR n_AMR n_EAS n_EUR n_SAS Gst Htmax Gstmax Gprimest ## 1 1320 694 1008 1008 978 0.557253 0.8286973 0.804979 0.6922578 We can use the GGV browser to visualize this variant’s allele frequencies across the 1000 Genomes populations. One complicating factor is that the browser is based on the hg19 reference genome, while we’re working with a VCF on the more recent hg38 coordinate system. How do you compare variants between genome builds? There are many ways of converting from genome coordinate system to the other. One way is to look up the ID of the SNP in the UCSC Genome Browser, using its chromosome and position: Fig. 4. Identifying a SNP’s rsID in the UCSC Genome Browser. This SNP has the ID rs7276293, which should be consistent between reference builds. "],["viewing-gst-hits-in-ggv.html", "8.11 Viewing GST hits in GGV", " 8.11 Viewing GST hits in GGV Now we can navigate to the GGV browser and look up the distribution of allele frequencies for this variant. Note that the “position” has changed due to the new coordinate system, but we are looking at the same SNP. This variant has huge allele frequency differences across populations, consistent with the high GST value that we computed. Also note that two populations in the Americas also carry this variant at high frequency. These are both African American populations – African Caribbean in Barbados (ACB) and African Ancestry in SW USA (ASW). Fig. 5. Allele frequencies of our top GST hit. "],["population-branch-statistic.html", "8.12 Population branch statistic", " 8.12 Population branch statistic \\(\\textrm{F}_{ST}\\) and related statistics ask about allele frequency differences between two populations. If we compare sets of three populations instead of focusing on pairs, we can calculate a different statistic called the population branch statistic (PBS). What are the advantages of comparing three populations? Two-population comparisons tell us that an allele frequency change happened after the two populations diverged, but give us no information about when it occurred. Comparing between three populations lets us contrast allele frequencies among the populations to localize frequency changes to a particular branch of a tree. Fig. 6. A variant exists at different frequencies in populations A and B. By comparing to a third population, we can identify that selection likely occurred within population A. "],["calculating-pbs.html", "8.13 Calculating PBS", " 8.13 Calculating PBS PBS is based on calculating \\(\\textrm{F}_{ST}\\) for each pairwise comparison of the three populations. We then calculate the branch length (\\(\\mathbf{T}\\)) that separates each population pair: \\[ T = -\\mathrm{log}(1 - \\textrm{F}_{ST}) \\] PBS is then computed as: \\[ \\textrm{PBS} = \\frac{\\textrm{T}_{AB} + \\textrm{T}_{AC} - \\textrm{T}_{BC}}{2} \\] "],["data-for-pbs.html", "8.14 Data (for PBS)", " 8.14 Data (for PBS) We’ll calculate PBS using data from the paper: Tucci, S. et al. (2018). Evolutionary history and adaptation of a human pygmy population of Flores Island, Indonesia.. Flores Island was home to fossils of the archaic hominin species Homo floresiensis, also called “hobbits” because their skeletons are 3 ft. tall. It’s thought that H. floresiensis lived there until just 40k-30k years ago, overlapping with modern humans. This study collected samples from individuals who currently live on Flores Island and scanned their genomes for evidence of natural selection. They calculated PBS between: The Rampasasa (RPS) population from Flores A Han Chinese (CHB) population A Melanesian population from Papua New Guinea (PNG) Fig. 7. Distribution of populations used in Tucci et al. "],["reading-in-pbs-data.html", "8.15 Reading in PBS data", " 8.15 Reading in PBS data We’ve pre-calculated pairwise \\(\\textrm{F}_{ST}\\) between the Rampasasa, Han Chinese, and Papua New Guinea populations. Load the results for chromosome 11 into R: fst_results &lt;- read.table(&quot;fst_results.txt.gz&quot;, header = TRUE) head(fst_results) ## chr pos rps.af chb.af png.af fst.rps.chb fst.rps.png ## 1 11 100001950 0.1666670 0.0145631 0.0428571 0.32524800 0.0845486 ## 2 11 100003476 0.0555556 0.0339806 0.1000000 -0.01078610 -0.0118052 ## 3 11 100004351 0.1111110 0.0485437 0.1142860 0.05945300 -0.0322152 ## 4 11 100005864 0.6111110 0.6553400 0.4285710 -0.02752900 0.0520164 ## 5 11 100006486 0.8333330 0.7766990 0.7571430 0.00768713 -0.0119055 ## 6 11 100006861 0.5000000 0.3834950 0.2285710 0.05413630 0.1807550 ## fst.png.chb ## 1 0.00465910 ## 2 0.03640770 ## 3 0.02896550 ## 4 0.11842600 ## 5 -0.00951234 ## 6 0.06338550 Every row of this table is a SNP: Columns ending in .af contain the allele frequency for that population Columns starting with fst. contain the \\(\\textrm{F}_{ST}\\) between that population pair "],["calculating-pbs-1.html", "8.16 Calculating PBS", " 8.16 Calculating PBS Using these \\(\\textrm{F}_{ST}\\) values, we can calculate (for every SNP) the branch lengths (\\(\\mathbf{T}\\)) that separate each population pair, and then calculate PBS. The mutate() function tidyverse’s mutate function is an easy way to perform calculations on tables. Its syntax is: mutate(new_column = &lt;formula for calculating column values&gt;) For example, if I wanted to create a new column that average each SNP’s allele frequencies across the three populations: fst_results %&gt;% mutate(avg_af = (rps.af + chb.af + png.af) / 3) Filling in the gaps in the code block below, use mutate to calculate T and PBS on the FST values: pbs &lt;- fst_results %&gt;% # calculate branch lengths between populations mutate(T_rps_chb = _________, T_rps_png = _________, T_png_chb = _________,) %&gt;% # calculate pbs mutate(pbs = _________) %&gt;% # sort by descending pbs value arrange(-pbs) Solution pbs &lt;- fst_results %&gt;% # calculate branch lengths between populations mutate(T_rps_chb = -log(1 - fst.rps.chb), T_rps_png = -log(1 - fst.rps.png), T_png_chb = -log(1 - fst.png.chb)) %&gt;% # calculate pbs mutate(pbs = ((T_rps_png + T_rps_chb) - (T_png_chb)) / 2) %&gt;% # sort by descending pbs value arrange(-pbs) head(pbs) ## chr pos rps.af chb.af png.af fst.rps.chb fst.rps.png fst.png.chb ## 1 11 126880301 0.833333 0.296117 0.0571429 0.734021 0.823884 0.216269 ## 2 11 126883747 0.833333 0.296117 0.0571429 0.734021 0.823884 0.216269 ## 3 11 126893266 0.833333 0.300971 0.0571429 0.729450 0.823884 0.222408 ## 4 11 126883622 0.833333 0.300971 0.0571429 0.729227 0.823884 0.221887 ## 5 11 126888750 0.833333 0.305825 0.0571429 0.724490 0.823884 0.227770 ## 6 11 126885142 0.833333 0.320388 0.0571429 0.709721 0.823884 0.244928 ## T_rps_chb T_rps_png T_png_chb pbs ## 1 1.324338 1.736612 0.2436894 1.408630 ## 2 1.324338 1.736612 0.2436894 1.408630 ## 3 1.307298 1.736612 0.2515533 1.396179 ## 4 1.306474 1.736612 0.2508835 1.396102 ## 5 1.289131 1.736612 0.2584728 1.383635 ## 6 1.236913 1.736612 0.2809422 1.346291 "],["manhattan-plot-of-pbs-results.html", "8.17 Manhattan plot of PBS results", " 8.17 Manhattan plot of PBS results We can visualize our PBS results by generating a Manhattan plot where we plot PBS on the y-axis instead of p-values. Make this Manhattan plot using geom_point. ggplot(data = pbs, aes(x = pos, y = pbs)) + geom_point() As evidence of positive selection, we are interested in both the height of the peaks in the plot, as well as the number of SNPs that comprise each peak (which is a proxy for haplotype length). "],["top-pbs-hits.html", "8.18 Top PBS hits", " 8.18 Top PBS hits What are the top PBS hits we’ve identified? Click on the pbs table to scroll through the SNPs with the highest PBS values. Use the UCSC Genome Browser to look up the top two PBS peaks. (We’re using the hg19 browser here because this data was generated with the hg19 reference genome.) What genes (if any) do these top PBS peaks overlap with? The top chr11 peak doesn’t seem to overlap with any genes, but is closest to KIRREL3. The second chr11 peak overlaps with the fatty acid desaturase gene cluster (FADS1, FADS2, and FADS3), which have previously been implicated as targets of independent episodes of positive selection in human populations. Fig. 8. The FADS gene cluster around the second PBS peak. "],["plotting-pbs-trees.html", "8.19 Plotting PBS trees", " 8.19 Plotting PBS trees Another useful way to visualize PBS is by comparing trees for the top PBS outliers to the genome-wide average tree. Run the code blocks below to plot these trees: # create average tree tr_mean &lt;- rtree(n = 3, rooted = FALSE, br = c(mean(pbs$T_rps_png), mean(pbs$T_rps_chb), mean(pbs$T_png_chb))) # plot average tree ggtree(tr_mean, layout = &quot;daylight&quot;) + geom_treescale(width = 0.1) + geom_tiplab(label = c(&quot;RPS&quot;, &quot;PNG&quot;, &quot;CHB&quot;)) ## Average angle change [1] 0.407407407407407 ## Average angle change [2] 0 # create tree for top snp tr_top &lt;- rtree(n = 3, rooted = FALSE, br = c(pbs[1,]$T_rps_png, pbs[1,]$T_rps_chb, pbs[1,]$T_png_chb)) # plot top snp tree ggtree(tr_top, layout = &quot;daylight&quot;) + geom_treescale(width = 0.1) + geom_tiplab(label = c(&quot;RPS&quot;, &quot;PNG&quot;, &quot;CHB&quot;)) ## Average angle change [1] 0.407407407407407 ## Average angle change [2] 0 "],["extended-haplotype-homozygosity.html", "8.20 Extended haplotype homozygosity", " 8.20 Extended haplotype homozygosity The haplotype-based class of selection statistics quantifies long haplotypes that result from a selective sweep. Extended haplotype homozygosity (EHH) is defined as the probability that any two haplotypes in a population are homozygous at SNPs in specific genomic region. EHH should be elevated in regions under historical selection, because a sweep causes one haplotype to rise to high frequency, and decay as you move further from the site under selection. Fig. 9 (from David Reich). Recombination breaks down the haplotype under selection (red) as you move further from the core variant. "],["plotting-ehh.html", "8.21 Plotting EHH", " 8.21 Plotting EHH EHH can be visualized with phylogeny-like plots like the one below. The width of the blue bar (the haplotype of interest) represents the haplotype’s frequency in a population, and the steps in the plot indicate historical recombination. The other branches of the phylogeny (in red) indicate other haplotypes in this region. An overrepresentation of one haplotype over the others creates the observation of extreme EHH. Fig. 10 (from David Reich). Overrepresentation of the blue haplotype indicates extreme EHH, a possible signature of selection. EHH plot for real data This plot shows EHH calculated for the CEU population of 1000 Genomes (Utah Residents [CEPH] with Northern and Western European Ancestry), zoomed in at the lactase (LCT) locus. A haplotype in this locus underwent a selective sweep in ancestor of European populations that allowed digestion of milk into adulthood, resulting in a present-day signature of EHH that stretches across megabases of sequence. In contrast, the ancestral haplotype (bottom) displays a much greater diversity and recombination. Fig. 11. EHH calculated for the lactase locus. "],["integrated-haplotype-statistic.html", "8.22 Integrated haplotype statistic", " 8.22 Integrated haplotype statistic If you calculate EHH for both the new and ancestral haplotypes at a locus, you can compare them to calculate the integrated haplotype statistic (iHS), which is the ratio of the area under the EHH curve for the derived and ancestral haplotypes. Fig. 12. Comparing the EHH of two alleles to calculate iHS. "],["the-pophuman-browser.html", "8.23 The PopHuman browser", " 8.23 The PopHuman browser While some R packages exist to compute EHH and iHS (e.g., the rehh package), they require some tricky processing of VCF files. Moreover, these statistics have already been computed genome-wide for all of the populations in the 1000 Genomes dataset, available from the PopHuman browser. This browser lets you pull up results for several population genetic signatures. Look up the lactase (LCT) gene in the search bar, and then click Select tracks in the upper left-hand corner to choose: iHS for the CEU (European) population pi for the CEU population. This is a measure of nucleotide diversity, which we expect to be low in a region under historical selection Fig. 13. iHS and \\(\\pi\\) in the LCT region. Note that if we compare both of these statistics to the genome-wide mean (yellow lines), we can see how dramatically they deviate from expectations in this genomic region. "],["conclusion-6.html", "8.24 Conclusion", " 8.24 Conclusion In this lab, we used three approaches to identify selection in multi-population sequencing data. Using data from the 1000 Genomes Project, we calculated FST, a measure of how different a variant’s allele frequency is between populations. We confirmed in the GGV Browser that the top FST variant shows strong population-specific AF differences. We then calculated the population branch statistic (PBS) to identify variants under selection in a human population on Flores Island, Indonesia. One of the top PBS hits was in the fatty acid desaturase gene cluster (FADS). Finally, we discussed extended haplotype homozygosity (EHH) and related statistics, which detect long haplotypes that result from a selective sweep. Using the PopHuman browser, we saw that the LCT locus – the most famous example of selection in humans – exhibits both elevated EHH and reduced genetic diversity (\\(\\pi\\)). "],["homework-7.html", "8.25 Homework", " 8.25 Homework 8.25.0.1 Learning Objectives Interpret multiple statistics for measuring selection Explain how specific statistics can give different results because they measure different genetic signatures 8.25.0.2 Assignment Read this review paper on genomic evidence of human local adaptation by Fan et al. Find examples of local adaptation (genes and populations) in the paper, and look up the relevant populations (or related populations) and tests of selection in the PopHuman browser. Are the signatures of selection apparent? Are the signature apparent based on all statistics? Why do certain statistics capture evidence of selection at certain loci but not others? "],["archaic-admixture.html", "9 Archaic admixture", " 9 Archaic admixture In this lab, we’ll discuss three common statistics, \\(D\\), \\(f_{4}\\), and the \\(f_{4}\\)-ratio, which are used for quantifying and testing hypotheses related to admixture (or “introgression”). 9.0.0.1 Learning objectives After completing this chapter, you’ll be able to: Explain how incomplete lineage sorting can create patterns of allele sharing that are discordant with species relationships. Describe how introgression is expected to affect patterns of allele sharing. Interpret the \\(D\\) and \\(f_4\\) statistics and their associated p-values. Interpret the \\(f_{4}\\)-ratio ratio statistic and whether it implies differences in archaic admixture between populations. "],["neanderthal-and-denisovan-introgression.html", "9.1 Neanderthal and Denisovan introgression", " 9.1 Neanderthal and Denisovan introgression As modern humans migrated out of Africa 50,000-70,000 years ago, they encountered and intebred with two groups of archaic hominins, the Neanderthals and Denisovans. Admixture with archaic hominins resulted in introgression of archaic DNA into these migrating populations. These Neanderthal and Denisovan sequences still persist in the genomes of their present-day descendents. Fig. 1. Ancient encounters between migrating human populations and archaic hominins. "],["inferring-introgression-from-phylogenetic-trees.html", "9.2 Inferring introgression from phylogenetic trees", " 9.2 Inferring introgression from phylogenetic trees One test for introgression – called the \\(\\textbf{D}\\) statistic, or ABBA-BABA test – compares the phylogenetic tree of species-level relationships with the tree for a specific genetic variant. We expect the species-level tree for Neanderthals and humans to look like the white bars of the tree in Fig. 2, where any two human populations are more similar to each other than they are to Neanderthals or chimpanzees (the outgroup). If a genetic variant differs between humans and Neanderthals, we generally expect it to segregate according to the species-level tree, in an AABB pattern (Fig. 2). Fig. 2. A variant concordant with the species-level tree for modern humans and Neanderthals. "],["incomplete-lineage-sorting.html", "9.3 Incomplete lineage sorting", " 9.3 Incomplete lineage sorting More rarely, the tree constructed for a specific SNP is discordant with the species tree. This is usually due to incomplete lineage sorting (ILS) – when genetic variation is present in the ancestor of two populations (e.g., the modern human–Neanderthal ancestor), then randomly fixes in a fashion that happens to conflict with the populations’ evolutionary relationships. ILS can create discordant ABBA or BABA trees for a given SNP. Under ILS, we would expect to see approximately equal numbers of ABBA and BABA trees. Fig. 3. ABBA and BABA allele patterns can be formed by incomplete lineage sorting. "],["evidence-of-introgression.html", "9.4 Evidence of introgression", " 9.4 Evidence of introgression Discordant trees can also be produced by introgression. In this case, the B allele arises on the Neanderthal lineage and is passed from Neanderthals into a human population, creating either an ABBA or BABA pattern (depending on which population receives Neanderthal gene flow). Fig. 4. ABBA and BABA allele patterns formed by Neanderthal introgression. "],["the-d-statistic.html", "9.5 The \\(D\\) statistic", " 9.5 The \\(D\\) statistic Introgression creates an imbalance in the number of ABBA or BABA trees if only one of the human populations has admixed with Neanderthals. The \\(\\textbf{D}\\) statistic quantifies this imbalance: \\[ D = \\frac{\\textrm{# BABA sites} - \\textrm{# ABBA sites}}{\\textrm{# BABA sites} + \\textrm{# ABBA sites}} \\] \\(D &gt; 0\\) is evidence for Neanderthal gene flow into the H2 population, while \\(D &lt; 0\\) is evidence for gene flow into H1. Choice of populations for the \\(D\\) statistic The choice of populations is very important when calculating the \\(D\\) statistic. When assessing archaic introgression, H2 is typicaly set as a human population without archaic admixture (e.g., a population from Africa). If H2 instead were a European population that did possess introgressed sequence, we would not expect a significant \\(D\\) statistic. "],["setup-5.html", "9.6 Setup", " 9.6 Setup In this lab, we’ll quantify evidence for introgression in human populations using \\(D\\) and related statistics. 9.6.1 R packages In addition to tidyverse, we’ll use the admixr package, which allows us to easily run the software package ADMIXTOOLS from within R to calculate \\(D\\) and related statistics. The exercises for class were adapted from the admixr tutorial, available here. library(tidyverse) library(admixr) "],["data-3.html", "9.7 Data", " 9.7 Data The admixr package provides real example data from 10 human individuals, which can be acquired by running its download_data() function: # download data into current directory prefix &lt;- download_data(dirname = &quot;.&quot;) We now have a directory called snps that contain four files: snps.geno: Genotype of each individual (column) at each SNP (row) Represented as counts of the alternative allele (0, 1, 2) snps.ind: Population IDs for each individual snps.snp: SNP IDs, positions, and alleles regions.bed: A file of genomic regions (not required for basic admixr analysis) EIGENSTRAT format Together, the three .geno, .ind, and .snp files constitute EIGENSTRAT format. This is just a way of representing genotype data, similar to a VCF – in fact, several software packages exist to convert between VCF and EIGENSTRAT. "],["reading-in-data.html", "9.8 Reading in data", " 9.8 Reading in data We can provide the location of the downloaded files to the eigenstrat() function, which then constructs an EIGENSTRAT object to be used for downstream analysis. # read in eigenstrat files snps &lt;- eigenstrat(prefix) snps ## EIGENSTRAT object ## ================= ## components: ## ind file: ./snps/snps.ind ## snp file: ./snps/snps.snp ## geno file: ./snps/snps.geno "],["the-d-function.html", "9.9 The d() function", " 9.9 The d() function admixr computes the \\(D\\) statistic in a function called d(). ?d As input, d() asks for: Data object in EIGENSTRAT format Four population names (W, X, Y, Z) to calculate D between "],["computing-the-d-statistic.html", "9.10 Computing the D statistic", " 9.10 Computing the D statistic Let’s compute \\(D\\) for four of the individuals we have data for: French, Sardinian, Vindija (Neanderthal), and chimpanzee. d_result &lt;- d(data = snps, # provide population names to calculate D between W = &quot;French&quot;, X = &quot;Sardinian&quot;, Y = &quot;Vindija&quot;, Z = &quot;Chimp&quot;) d_result ## W X Y Z D stderr Zscore BABA ABBA nsnps ## 1 French Sardinian Vindija Chimp 0.0038 0.0074 0.511 10974 10891 487843 How do we interpret these results? The last three columns count the number of ABBA and BABA sites, as well as the total number of variants being analyzed. First, note that the ABBA/BABA sites are only a small fraction of the total number of variants – most variants conform to the species-level tree. The number of ABBA and BABA variants also looks similar, which implies that the discordant trees in these four populations primarily result from ILS rather than introgression. The middle columns give the actual value of \\(D\\) and its standard error, as well as the Z score (which is equal to \\(\\frac{D}{\\textrm{stderr}}\\)). "],["converting-to-p-values.html", "9.11 Converting to p-values", " 9.11 Converting to p-values How do we know whether the ABBA-BABA counts are significantly different? We can convert the Z score into a p-value: d_result &lt;- d_result %&gt;% # convert z score into pvalue mutate(p = 2 * pnorm(-abs(Zscore))) d_result ## W X Y Z D stderr Zscore BABA ABBA nsnps ## 1 French Sardinian Vindija Chimp 0.0038 0.0074 0.511 10974 10891 487843 ## p ## 1 0.6093511 Interpretation of this p-value The p-value does not look significant (no introgression into the ancestors of this French or Sardinian individual). This is in line with our observation that there doesn’t seem to be a disproportionate amount of ABBA or BABA sites. "],["computing-d-for-all-populations.html", "9.12 Computing D for all populations", " 9.12 Computing D for all populations We can give d() a list of populations to compute the \\(D\\) statistic for, where: W: A vector of our populations of interest X: An assumed unadmixed population (Yoruba) Y: The introgressing population (Vindija Neanderthal) Z: Outgroup (chimpanzee) # create vector of populations of interest pops &lt;- c(&quot;French&quot;, &quot;Sardinian&quot;, &quot;Han&quot;, &quot;Papuan&quot;, # including three African populations &quot;Khomani_San&quot;, &quot;Mbuti&quot;, &quot;Dinka&quot;) # calculate D d_result &lt;- d(data = snps, W = pops, X = &quot;Yoruba&quot;, Y = &quot;Vindija&quot;, Z = &quot;Chimp&quot;) %&gt;% # convert z score into pvalue mutate(p = 2 * pnorm(-abs(Zscore))) d_result ## W X Y Z D stderr Zscore BABA ABBA nsnps ## 1 French Yoruba Vindija Chimp 0.0313 0.006933 4.510 15802 14844 487753 ## 2 Sardinian Yoruba Vindija Chimp 0.0287 0.006792 4.222 15729 14852 487646 ## 3 Han Yoruba Vindija Chimp 0.0278 0.006609 4.199 15780 14928 487925 ## 4 Papuan Yoruba Vindija Chimp 0.0457 0.006571 6.953 16131 14721 487694 ## 5 Khomani_San Yoruba Vindija Chimp 0.0066 0.006292 1.051 16168 15955 487564 ## 6 Mbuti Yoruba Vindija Chimp -0.0005 0.006345 -0.074 15751 15766 487642 ## 7 Dinka Yoruba Vindija Chimp -0.0009 0.006124 -0.151 15131 15159 487667 ## p ## 1 6.482763e-06 ## 2 2.421441e-05 ## 3 2.680963e-05 ## 4 3.575987e-12 ## 5 2.932586e-01 ## 6 9.410104e-01 ## 7 8.799757e-01 Interpretation of \\(D\\) results This is the result that was published in Green et al. 2010, revealing evidence for gene flow from Neanderthals into the ancestors of non-African populations. We see significant evidence of Neanderthal introgression into the genomes of the non-African samples (French, Sardinian, Han, Papuan), but not the African samples (although we know from recent research that this is an oversimplification). "],["plotting-the-d-statistic.html", "9.13 Plotting the D statistic", " 9.13 Plotting the D statistic Use this code to plot the \\(D\\) statistic and standard error calculated for each population. This is a visual representation of the data in the d_result table: ggplot(d_result, aes(fct_reorder(W, D), D, color = p &lt; 0.05)) + geom_point() + geom_hline(yintercept = 0, linetype = 2) + geom_errorbar(aes(ymin = D - 1.96 * stderr, ymax = D + 1.96 * stderr), width = 0.5) + xlab(&quot;Population&quot;) "],["f_4-statistic.html", "9.14 \\(f_{4}\\) statistic", " 9.14 \\(f_{4}\\) statistic The \\(\\mathbf{f_{4}}\\) statistic – not to be confused with the \\(\\mathrm{F_{ST}}\\) from the previous week – is very similar to the D statistic. Its main advantage is that it is proportional to the branch length separating two pairs of populations. Compute the \\(f_{4}\\) statistic for all populations using the code below: f4_result &lt;- f4(data = snps, W = pops, X = &quot;Yoruba&quot;, Y = &quot;Vindija&quot;, Z = &quot;Chimp&quot;) %&gt;% # convert z score into pvalue mutate(p = 2 * pnorm(-abs(Zscore))) f4_result ## W X Y Z f4 stderr Zscore BABA ABBA nsnps ## 1 French Yoruba Vindija Chimp 0.001965 0.000437 4.501 15802 14844 487753 ## 2 Sardinian Yoruba Vindija Chimp 0.001798 0.000427 4.209 15729 14852 487646 ## 3 Han Yoruba Vindija Chimp 0.001746 0.000418 4.178 15780 14928 487925 ## 4 Papuan Yoruba Vindija Chimp 0.002890 0.000417 6.924 16131 14721 487694 ## 5 Khomani_San Yoruba Vindija Chimp 0.000436 0.000415 1.051 16168 15955 487564 ## 6 Mbuti Yoruba Vindija Chimp -0.000030 0.000410 -0.074 15751 15766 487642 ## 7 Dinka Yoruba Vindija Chimp -0.000057 0.000380 -0.151 15131 15159 487667 ## p ## 1 6.763451e-06 ## 2 2.565034e-05 ## 3 2.940837e-05 ## 4 4.390659e-12 ## 5 2.932586e-01 ## 6 9.410104e-01 ## 7 8.799757e-01 Note that the p-values are the same as when we calculated the \\(D\\) statistic, but the actual \\(f_4\\) values are different. "],["f_4-ratio-statistic.html", "9.15 \\(f_{4}\\)-ratio statistic", " 9.15 \\(f_{4}\\)-ratio statistic The branch length proportionality of the \\(f_4\\) statistic is useful for deriving yet another statistic, called the \\(\\mathbf{f_{4}}\\)-ratio statistic. As implied by its name, this simply a ratio of two different \\(f_{4}\\) statistics. Unlike \\(D\\) and \\(f_{4}\\), the \\(f_{4}\\)-ratio tells us how much Neanderthal ancestry a given individual possesses. Calculate the \\(f_{4}\\)-ratio using the code block below: f4_ratio_result &lt;- f4ratio(data = snps, X = pops, A = &quot;Altai&quot;, B = &quot;Vindija&quot;, C = &quot;Yoruba&quot;, O = &quot;Chimp&quot;) %&gt;% # convert z score to pvalue mutate(p = 2 * pnorm(-abs(Zscore))) f4_ratio_result ## A B X C O alpha stderr Zscore p ## 1 Altai Vindija French Yoruba Chimp 0.023774 0.006176 3.850 1.181178e-04 ## 2 Altai Vindija Sardinian Yoruba Chimp 0.024468 0.006071 4.031 5.554004e-05 ## 3 Altai Vindija Han Yoruba Chimp 0.022117 0.005892 3.754 1.740349e-04 ## 4 Altai Vindija Papuan Yoruba Chimp 0.037311 0.005812 6.420 1.362743e-10 ## 5 Altai Vindija Khomani_San Yoruba Chimp 0.003909 0.005913 0.661 5.086123e-01 ## 6 Altai Vindija Mbuti Yoruba Chimp 0.000319 0.005717 0.056 9.553418e-01 ## 7 Altai Vindija Dinka Yoruba Chimp -0.001500 0.005394 -0.278 7.810124e-01 For this statistic, alpha represents the proportion of the genome whose ancestry traces to Neanderthal introgression. "],["plotting-f_4-ratio-results.html", "9.16 Plotting \\(f_{4}\\)-ratio results", " 9.16 Plotting \\(f_{4}\\)-ratio results Run the code below to plot the \\(f_{4}\\)-ratio values we computed: ggplot(f4_ratio_result, aes(fct_reorder(X, alpha), alpha, color = p &lt; 0.05)) + geom_point() + geom_errorbar(aes(ymin = alpha - 2 * stderr, ymax = alpha + 2 * stderr), width = 0.5) + geom_hline(yintercept = 0, linetype = 2) + labs(y = &quot;Neanderthal ancestry proportion&quot;, x = &quot;Present-day individual&quot;) Based on what we know about Papuan populations, do you think the estimate of 4% Neanderthal introgressed DNA is accurate? We know that Oceanian populations also experienced introgression from Denisovans. Denisovans were more genetically similar to Neanderthals than to modern humans, so Denisovan ancestry is being counted as Neanderthal ancestry by this metric. "],["computing-statistics-in-genomic-intervals.html", "9.17 Computing statistics in genomic intervals", " 9.17 Computing statistics in genomic intervals We’ve so far computed the \\(D\\), \\(f_4\\), and \\(f_4\\)-ratio statistics across the entire genome, but we can also restrict computation to particular genomic intervals, such as genes, promoters, enhancers, etc. What would region-specific differences imply? Region-specific differences suggest that there are some regions of the genome that are particularly tolerant or intolerant to introgression. Fig. 5 (source). Proportion of introgressed Neanderthal ancestry within specific gene annotations. In the figure below, we see that the genome-wide average percentage of Neanderthal ancestry is ~2%, but there are some genomic regions (promoters, evolutionarily conserved sequences) where this percentage is lower. One theory explaining this pattern is that Neanderthals accumulated a genetic load of slightly deleterious mutations due to their small population sizes. Consequently, when Neanderthal sequences were introgressed into human populations, they were subjected to negative selection, which was stronger in more functionally important regions of the genome. "],["bed-files.html", "9.18 BED files", " 9.18 BED files Annotations of genomic intervals are commonly represented in a file format called BED. We can utilize these files in admixr’s filter_bed() function, which excludes or restricts analyses to genomic intervals within a BED file. Run the code below to re-calculate the \\(f_4\\)-ratio with regions.bed. We can create a new snps data object that either keeps or excludes these regions from our analysis. # get the path to the `regions.bed` file bed &lt;- file.path(dirname(prefix), &quot;regions.bed&quot;) # option 1: KEEP only these regions for analysis new_snps_keep &lt;- filter_bed(snps, bed) # option 2: REMOVE these regions from analysis new_snps_remove &lt;- filter_bed(snps, bed, remove = TRUE) "],["region-specific-f_4-ratio.html", "9.19 Region-specific \\(f_4\\) ratio", " 9.19 Region-specific \\(f_4\\) ratio Now we can re-calculate the \\(f_4\\)-ratio on one of the filtered SNP sets. # f4-ratio with the regions removed f4_filtered &lt;- f4ratio(data = new_snps_remove, X = pops, A = &quot;Altai&quot;, B = &quot;Vindija&quot;, C = &quot;Yoruba&quot;, O = &quot;Chimp&quot;) %&gt;% # convert z score to pvalue mutate(p = 2 * pnorm(-abs(Zscore))) f4_filtered ## A B X C O alpha stderr Zscore p ## 1 Altai Vindija French Yoruba Chimp 0.016554 0.007929 2.088 3.679783e-02 ## 2 Altai Vindija Sardinian Yoruba Chimp 0.027980 0.007826 3.575 3.502279e-04 ## 3 Altai Vindija Han Yoruba Chimp 0.020285 0.007348 2.761 5.762468e-03 ## 4 Altai Vindija Papuan Yoruba Chimp 0.036136 0.007550 4.786 1.701381e-06 ## 5 Altai Vindija Khomani_San Yoruba Chimp 0.005713 0.007593 0.752 4.520511e-01 ## 6 Altai Vindija Mbuti Yoruba Chimp 0.009803 0.007233 1.355 1.754176e-01 ## 7 Altai Vindija Dinka Yoruba Chimp -0.000663 0.006798 -0.097 9.227264e-01 Plot the region-excluded \\(f_4\\)-ratios ggplot(f4_filtered, aes(fct_reorder(X, alpha), alpha, color = p &lt; 0.05)) + geom_point() + geom_errorbar(aes(ymin = alpha - 2 * stderr, ymax = alpha + 2 * stderr), width = 0.5) + geom_hline(yintercept = 0, linetype = 2) + labs(y = &quot;Neanderthal ancestry proportion&quot;, x = &quot;Present-day individual&quot;) "],["conclusion-7.html", "9.20 Conclusion", " 9.20 Conclusion In this lab, we assessed evidence for Neanderthal introgression into specific human populations. We reviewed the \\(\\mathbf{D}\\) statistic, which asks whether a variant shows an imbalance in the population inheritance patterns expected from incomplete lineage sorting (ILS). Such an imbalance implies historical introgression into one population. With admixr, we calculated the \\(D\\) statistic for one individual from each of seven modern human populations, identifying significant evidence for Neanderthal introgression into the ancestors of non-African – but not African – populations. We also calculated the \\(\\mathbf{f_4}\\) statistic, which is very similar to the \\(D\\) statistic but is proportional to the genetic distance between two populations. Finally, we computed the \\(\\mathbf{f_4}\\)-ratio, whose value indicates the proportion of introgressed ancestry in each population. We observed a higher proportion of Neanderthal ancestry in the Papuan individual in our dataset, which is likely an artifact of Denisovan introgression into Oceanian populations. "],["homework-8.html", "9.21 Homework", " 9.21 Homework 9.21.0.1 Learning Objectives Practice calculating introgression statistics in admixr Interpret the biological significance of region-specific values of the \\(f_4\\)-ratio 9.21.0.2 Assignment Follow these steps to create your own genome stratifications for calculating the \\(f_4\\) ratio statistic. Go to the UCSC Table Browser, where you can find a wide selection of annotations for the human genome. Make sure you set the assembly: drop-down box to Feb. 2009 (GRCh37/hg19). Use the group: and track: menus to select any set of genomic regions. You can click the data format description button and scroll to the Description section to find out what each annotation represents. Under the Retrieve and display data section, set the output format: to BED. Enter an output filename: (ex: all_genes.bed). Click get output to download the file. In Posit Cloud, upload your file using the Upload button in the Files panel (bottom right). Run the code block below to reformat the BED file. The code matches the UCSC’s chromosome naming format with the format used in the snps data: # fill in blank with the name of your bed file system(command = &quot;sed -i &#39;s/chr//g&#39; ________&quot;) # get the path to your bed file bed &lt;- file.path(&quot;________&quot;) Compute the \\(f_4\\) ratio statistic within and outside of the genomic intervals. Repeat for another set of genome annotations to contrast Neanderthal ancestry in different genomic elements. Solution Download tracklist of haploinsufficient genes (Phenotype and Literature -&gt; Haploinsufficiency). # get the path to the `regions.bed` file bed &lt;- file.path(&quot;haploinsufficient.bed&quot;) # option 1: KEEP only these regions for analysis new_snps_keep &lt;- filter_bed(snps, bed) # option 2: REMOVE these regions from analysis new_snps_remove &lt;- filter_bed(snps, bed, remove = TRUE) Re-calculate the \\(f_4\\)-ratio: # f4-ratio with the regions kept f4_keep &lt;- f4ratio(data = new_snps_keep, X = pops, A = &quot;Altai&quot;, B = &quot;Vindija&quot;, C = &quot;Yoruba&quot;, O = &quot;Chimp&quot;) %&gt;% # convert z score to pvalue mutate(p = 2 * pnorm(-abs(Zscore))) f4_keep ## A B X C O alpha stderr Zscore p ## 1 Altai Vindija French Yoruba Chimp 0.030580 0.009385 3.258 1.122004e-03 ## 2 Altai Vindija Sardinian Yoruba Chimp 0.018057 0.009117 1.981 4.759127e-02 ## 3 Altai Vindija Han Yoruba Chimp 0.023127 0.009617 2.405 1.617247e-02 ## 4 Altai Vindija Papuan Yoruba Chimp 0.036462 0.008964 4.068 4.741838e-05 ## 5 Altai Vindija Khomani_San Yoruba Chimp -0.001223 0.008774 -0.139 8.894501e-01 ## 6 Altai Vindija Mbuti Yoruba Chimp -0.015781 0.009100 -1.734 8.291808e-02 ## 7 Altai Vindija Dinka Yoruba Chimp -0.004929 0.008235 -0.599 5.491729e-01 # f4-ratio with the regions removed f4_remove &lt;- f4ratio(data = new_snps_remove, X = pops, A = &quot;Altai&quot;, B = &quot;Vindija&quot;, C = &quot;Yoruba&quot;, O = &quot;Chimp&quot;) %&gt;% # convert z score to pvalue mutate(p = 2 * pnorm(-abs(Zscore))) f4_remove ## A B X C O alpha stderr Zscore p ## 1 Altai Vindija French Yoruba Chimp 0.016554 0.007929 2.088 3.679783e-02 ## 2 Altai Vindija Sardinian Yoruba Chimp 0.027980 0.007826 3.575 3.502279e-04 ## 3 Altai Vindija Han Yoruba Chimp 0.020285 0.007348 2.761 5.762468e-03 ## 4 Altai Vindija Papuan Yoruba Chimp 0.036136 0.007550 4.786 1.701381e-06 ## 5 Altai Vindija Khomani_San Yoruba Chimp 0.005713 0.007593 0.752 4.520511e-01 ## 6 Altai Vindija Mbuti Yoruba Chimp 0.009803 0.007233 1.355 1.754176e-01 ## 7 Altai Vindija Dinka Yoruba Chimp -0.000663 0.006798 -0.097 9.227264e-01 Some of the alpha values for each population change when excluding/restricting to haploinsufficient genes, but their standard error ranges still overlap between the two \\(f_4\\)-ratio calculations, so they likely aren’t truly different. "],["simulating-evolution.html", "10 Simulating evolution", " 10 Simulating evolution In this lab, we’ll build a simulation to explore genetic drift using the Wright-Fisher model. 10.0.0.1 Learning objectives After completing this chapter, you’ll be able to: Describe the phenomenon of genetic drift. Explain why random draws from a binomial distribution are a good way to mimic the effect of drift. Interpret allele frequency patterns that occur as a result of drift. Write a for loop in R. Write a function to run code multiple times with different parameters. "],["genetic-drift.html", "10.1 Genetic drift", " 10.1 Genetic drift In all populations, genetic drift acts to change allele frequencies over time. Drift refers to random changes in an allele’s frequency between generations. These random changes occur because individuals carrying different alleles will have different numbers of offspring due to chance. Fig. 1 (source). An allele’s frequency “drifts” between generations due to random chance. Drift differs from selection, which is a deterministic (non-random) change in an allele’s frequency. If an allele is under selection, it’s more likely to increase or decrease in frequency depending on whether it is beneficial or deleterious. Genetic drift, on the other hand, cannot consistently cause an allele’s frequency to increase or decrease. "],["the-wright-fisher-model.html", "10.2 The Wright-Fisher model", " 10.2 The Wright-Fisher model The Wright-Fisher model is one of the most commonly used models of genetic drift. In this model, we assume that a population: Mates randomly Number of individuals remains constant between generations Today, we’ll also assume that the population is haploid Populations in the real world don’t behave like an ideal Wright-Fisher population, so their effective population size Ne is usually much smaller than their actual population size. The effective population size of the human population is only 12,800–14,400 individuals, even though its actual size is around 8 billion. "],["allele-frequency-fixation-and-loss.html", "10.3 Allele frequency, fixation, and loss", " 10.3 Allele frequency, fixation, and loss The Wright-Fisher model describes the behavior of a single allele, which can be at any variable site in a population (a SNP, insertion/deletion, version of a gene, etc.). The allele of interest begins the simulation at some initial allele frequency (AF). This allele frequency is the proportion of individuals in the population who carry that allele, and is always between 0 and 1. An allele becomes fixed in a population if it reaches an allele frequency of 1, and is lost if it reaches a frequency of 0. At either of these points, it is no longer considered a variable site because either everyone or no one in the population carries it. Fig. 2 (source). Trajectories of alleles at two loci fixing, at AF = 0 and AF = 1. "],["modeling-allele-frequencies.html", "10.4 Modeling allele frequencies", " 10.4 Modeling allele frequencies In the Wright-Fisher model, we track a population over the course of many generations. Within each generation: For every individual, we perform a coin flip to determine whether or not they have the allele. Unlike a coin, the probability of receiving an allele is equal to its allele frequency in the current generation. The more common an allele is in this generation, the more likely it is that someone in the next generation will also carry it. After flipping these coins, we know the number of people in the next generation who carry the allele. Fig. 3. Every individual flips a weighted coin to determine whether they will carry the blue allele in the next generation. The probability of carrying the allele is equal to the allele’s frequency in the current generation. "],["the-binomial-distribution.html", "10.5 The binomial distribution", " 10.5 The binomial distribution Instead of having to actually simulate all the coin flips, we can get the number of allele carriers by performing a single draw from a binomial distribution with size N (# of individual) and success probability p = AF. This distribution tells you how many successes you expect to see from a set of N independent coin flips. If we try to draw 100,000 times from a binomial distribution with population size 100 and success probability (AF) 0.5, it will look something like this: Figure 10.1: 100,000 draws from a binomial distribution. The majority of the distribution lies between 48 and 52. Just as we expect based on the allele frequency, the next generation will most likely have around 48-52 individuals with the A allele. But because this is a random draw, there’s a small chance that we might end up with many more or many fewer than that number. "],["setup-6.html", "10.6 Setup", " 10.6 Setup 10.6.1 R packages library(tidyverse) 10.6.2 Data We’ll simulate all of our own data for this lab! "],["the-rbinom-function.html", "10.7 The rbinom function", " 10.7 The rbinom function The basis of our simulation is R’s rbinom function, which allows us to sample from a binomial distribution. rbinom takes three arguments: n: how many times we’re drawing from the distribution size: the size of the population we’re sampling from (i.e. N) p: the success probability (i.e. allele frequency) Every generation, we’ll draw once to produce the number of individuals carrying the A allele in the next generation. Let’s once again look at a population of size 100, and an A allele currently at AF = 0.5. We use rbinom to get the number of individuals in the next generation who will have A: rbinom(n = 1, size = 100, prob = 0.5) ## [1] 45 Change the rbinom code so that it returns the allele frequency (instead of the number of individuals). # divide by the population size to get AF rbinom(n = 1, size = 100, prob = 0.5) / 100 ## [1] 0.51 Why do we get a different number every time we run rbinom? rbinom generates a random number between 0 and 100. Because it’s random, the number it draws will be different every time we run it. "],["increasing-population-size.html", "10.8 Increasing population size", " 10.8 Increasing population size Currently, we’re drawing from a population of 100 individuals. Now let’s see what happens when we increase the population size. (Feel free to run this code block multiple times!) rbinom(n = 1, size = 10000, prob = 0.5) / 10000 ## [1] 0.4934 If you run the code block above multiple times, you’ll observe that the AF is much closer to 0.5 than it was with a population of size 100. This lends to our intuition that an allele’s frequency fluctuates much more when a population is small, and is more stable when the population size is large. How does population size affect an allele’s time to fixation? As population size gets larger, the allele will take longer to fix. "],["simulating-multiple-generations.html", "10.9 Simulating multiple generations", " 10.9 Simulating multiple generations Currently, we draw once from a binomial distribution to get the number of individuals in one generation who carry the allele of interest. How do we adapt this to simulate multiple generations? Increasing n (ex: rbinom(n = 10, size = 100, prob = 0.5)) Increasing n only gives you multiple replicate draws from the same distribution. This doesn’t reflect multiple generations, because the allele frequency doesn’t update between generations based on the new number of alleles – it uses prob = 0.5 every time. "],["for-loops.html", "10.10 For loops", " 10.10 For loops Instead of drawing multiple times from the same distribution, we can write a for loop to repeatedly generate and update the number of individuals with the A allele. A for loop allows you to run some code X number of times. For example: for (i in 1:3) { print(i) } ## [1] 1 ## [1] 2 ## [1] 3 This for loop goes through all the values between 1 and 3, and prints each of them out. Modify the for loop to instead run our rbinom command. for (i in 1:3) { print(rbinom(n = 1, size = 100, prob = 0.5) / 100) } ## [1] 0.49 ## [1] 0.45 ## [1] 0.43 "],["updating-variables-within-a-for-loop.html", "10.11 Updating variables within a for loop", " 10.11 Updating variables within a for loop We also need to update the allele frequency in every iteration of the for loop. We do this by adding a freq variable that keeps track of the current AF: # start an initial AF of 0.5 freq &lt;- 0.5 for (i in 1:3) { # run rbinom to generate the AF for the next generation new_freq &lt;- rbinom(n = 1, size = 100, prob = freq) / 100 # print new AF print(new_freq) # update `freq` in each iteration of the loop freq &lt;- new_freq } ## [1] 0.53 ## [1] 0.51 ## [1] 0.52 "],["adding-a-population-size-variable.html", "10.12 Adding a population size variable", " 10.12 Adding a population size variable Using the freq variable also gives us more flexibility than hard-coding an allele frequency into the rbinom command. Add to your code so that we also provide Ne (effective population size) as a variable (without updating it in the for loop). # set effective population size outside of for loop Ne &lt;- 100 # start an initial AF of 0.5 freq &lt;- 0.5 for (i in 1:3) { # run rbinom to generate the AF for the next generation new_freq &lt;- rbinom(n = 1, size = Ne, prob = freq) / Ne # print new AF print(new_freq) # update `freq` in each iteration of the loop freq &lt;- new_freq } ## [1] 0.53 ## [1] 0.47 ## [1] 0.43 "],["changes-in-af-over-generations.html", "10.13 Changes in AF over generations", " 10.13 Changes in AF over generations Try increasing the number of generations we run the simulation for. What patterns of change do you observe in the allele frequencies? Ne &lt;- 100 freq &lt;- 0.5 for (i in 1:20) { new_freq &lt;- rbinom(n = 1, size = Ne, prob = freq) / Ne print(new_freq) freq &lt;- new_freq } ## [1] 0.53 ## [1] 0.44 ## [1] 0.41 ## [1] 0.48 ## [1] 0.55 ## [1] 0.49 ## [1] 0.5 ## [1] 0.5 ## [1] 0.44 ## [1] 0.48 ## [1] 0.51 ## [1] 0.46 ## [1] 0.45 ## [1] 0.48 ## [1] 0.46 ## [1] 0.51 ## [1] 0.55 ## [1] 0.58 ## [1] 0.54 ## [1] 0.56 The allele frequency starts approaching either 0 or 1. "],["storing-afs-in-a-vector.html", "10.14 Storing AFs in a vector", " 10.14 Storing AFs in a vector To plot how the AF changes over time, we can store the AF at each generation in a vector. Vectors R’s version of a list, and are formed with the c() function, which stands for “combine”: my_vec &lt;- c(0.5, 0.6) my_vec ## [1] 0.5 0.6 You can append elements to a vector called my_vec by running: my_vec &lt;- c(my_vec, new_element). Modify the code block with our for loop to create a vector for storing allele frequencies, and then append the updated AF to it every generation. We need to create the vector before the for loop, and then append to the vector within the for loop. Ne &lt;- 100 freq &lt;- 0.5 # create vector to store AFs in freq_vector &lt;- freq for (i in 1:20) { new_freq &lt;- rbinom(n = 1, size = Ne, prob = freq) / Ne # add new freq to the AF vector freq_vector &lt;- c(freq_vector, new_freq) freq &lt;- new_freq } freq_vector ## [1] 0.50 0.52 0.64 0.63 0.65 0.63 0.61 0.68 0.68 0.65 0.63 0.63 0.64 0.62 0.57 ## [16] 0.60 0.61 0.63 0.64 0.65 0.68 "],["reformatting-afs-for-plotting.html", "10.15 Reformatting AFs for plotting", " 10.15 Reformatting AFs for plotting Because ggplot requires its input data to be formatted as a table, we have to convert freq_vector into some form of table (ex: a tibble or dataframe). sim_results &lt;- tibble(af = freq_vector) head(sim_results) ## # A tibble: 6 × 1 ## af ## &lt;dbl&gt; ## 1 0.5 ## 2 0.52 ## 3 0.64 ## 4 0.63 ## 5 0.65 ## 6 0.63 This table contains the information that we want on the plot’s y axis. We can now add in a column containing the plot’s x axis data, which is the generation that each AF value corresponds to. sim_results &lt;- tibble(af = freq_vector, gen = 1:21) head(sim_results) ## # A tibble: 6 × 2 ## af gen ## &lt;dbl&gt; &lt;int&gt; ## 1 0.5 1 ## 2 0.52 2 ## 3 0.64 3 ## 4 0.63 4 ## 5 0.65 5 ## 6 0.63 6 Why does the gens column range from 1 to 21 (instead of 1 to 20)? We add our starting allele frequency to freq_vector, and then simulate for 20 generations. This means that we end up with 21 AFs in our vector. "],["plotting-af-trajectory.html", "10.16 Plotting AF trajectory", " 10.16 Plotting AF trajectory Plot the trajectory of AFs over time with ggplot. ggplot(data = sim_results, aes(x = gen, y = af)) + geom_line() "],["simulating-different-parameters-with-a-function.html", "10.17 Simulating different parameters with a function", " 10.17 Simulating different parameters with a function It would be nice to be able to run our Wright-Fisher simulation with different parameters – like different starting allele frequencies, population sizes, etc. – without having to edit the for loop code every time. We can use a function to generalize the code above so we can easily re-run it. The structure of an R function You’ve already encountered many functions in R, even if you didn’t realize it at the time - rbinom, ggplot, and print are all examples of functions. An R function has four parts: &lt;Name&gt; &lt;- function(&lt;Argument(s)&gt;) { &lt;Body&gt; &lt;return()&gt; } Name − The function is stored in your R environment as an object with this name, and you use the name to call it Argument(s) − Optional; input values that the function performs operations on Body − The code that describes what the function does Return − Optional; a return statement allows the function to return a value to the user. Without a return statement, you won’t be able to access the function’s output Here’s an example function that takes in three parameters for running rbinom, and returns the output of rbinom. binom_sim &lt;- function(myN, mySize, myProb) { output &lt;- rbinom(myN, mySize, myProb) return(output) } How do I know when to use a function? Functions are useful whenever you have code that you want to run multiple times with slightly different parameters. If you find yourself copying over code several times and changing just a few things, you should consider writing a function instead. "],["creating-a-wright-fisher-function.html", "10.18 Creating a Wright-Fisher function", " 10.18 Creating a Wright-Fisher function We want our function to take in parameters for the starting allele frequency, population size, and number of generations to simulate. It should return the sim_results dataframe so that we can plot the allele frequency trajectory. To write a function, we can put the code that we just wrote into the function body: run_sim &lt;- function(Ne, freq, generations) { # note how we don&#39;t define our initial parameters for Ne, freq, etc. # because we&#39;re passing in those parameters as arguments freq_vector &lt;- freq for (i in 1:generations) { new_freq &lt;- rbinom(n = 1, size = Ne, prob = freq) / Ne freq_vector &lt;- c(freq_vector, new_freq) freq &lt;- new_freq } # convert vector of AFs into a tibble for plotting sim_results &lt;- tibble(afs = freq_vector, gen = 1:(generations+1)) # return the tibble of AFs, so that we can access the results return(sim_results) } "],["running-a-function.html", "10.19 Running a function", " 10.19 Running a function The code block we just wrote defines the function (i.e., tells R what it should do). Now we can run the function with parameters of our choosing and plot the output: # run function results &lt;- run_sim(Ne = 1000, freq = 0.5, generations = 10000) # plot output ggplot(data = results, aes(x = gen, y = afs)) + geom_line() Run your run_sim function a few times with different input population sizes and AFs. How does changing these inputs affect the AF trajectories that you see? # simulate a very large population results &lt;- run_sim(Ne = 100000, freq = 0.5, generations = 1000) # plot output ggplot(data = results, aes(x = gen, y = afs)) + geom_line() # simulate a population with AF closer to 0 results &lt;- run_sim(Ne = 1000, freq = 0.1, generations = 1000) # plot output ggplot(data = results, aes(x = gen, y = afs)) + geom_line() In general, decreasing the population size or shifting the starting AF away from 0.5 will decrease the time to fixation for an allele. "],["conclusion-8.html", "10.20 Conclusion", " 10.20 Conclusion In this lab, we built a Wright-Fisher simulation for one allele, allowing us to track how we expect its frequency to change over time under the principles of genetic drift. Within each simulated generation, we drew the number of allele carriers in the next generation from a binomial distribution. We performed these AF draws within a for loop, allowing us to update the current AF with each new generation. Finally, we wrapped our Wright-Fisher simulation code into a function, allowing us to easily re-run our simulation with different parameters for the starting AF, population size, and number of generations to simulate for. We observed that smaller population sizes and more extreme AFs (closer to 0 or 1) generally decrease an allele’s time to fixation. This simple simulation forms the core of most models used in evolutionary genetics research, which often extend this model to simulate more complex phenomena (such as different forms of selection). "],["homework-9.html", "10.21 Homework", " 10.21 Homework One way to extend our simple Wright-Fisher model is to add in selection as a parameter. Selection affects our model by altering the probability of sampling our allele of interest each generation (e.g., positive selection increases the probability, and negative selection decreases it). Previously, we assumed that this probability was equivalent to the allele’s frequency, or \\(p = \\frac{i}{N_e}\\), where \\(N_e\\) is the population size and \\(i\\) is the number of individuals who carry the allele. For the purposes of this homework, we assume that in a model with selection, this probability is instead: \\[ p = \\frac{i(1 + s)}{N_e - i + i(1+s)} \\] where \\(s\\) is the selection coefficient, and ranges from -1 to 1. What does this probability become in the absence of selection (i.e., when \\(s = 0\\))? The probability becomes \\(\\frac{i}{N_e}\\), which is the same as the allele frequency. 10.21.0.1 Learning Objectives Practice writing functions in R Interpret allele frequency trajectories under selection and drift 10.21.0.2 Assignment In the code block below, modify your run_sim function so that it takes in a selection coefficient s as a parameter. Run the simulation a few times with and without (s = 0) selection, but keeping other parameters the same (Ne = 10000, freq = 0.5, generations = 10000). What do you notice about the allele frequency trajectories? Note that most selection coefficients are thought to be extremely small – the largest known selection coefficients in humans are around 0.05. Solution # simulation function with selection run_sim_selection &lt;- function(Ne, freq, generations, s) { freq_vector &lt;- freq for (i in 1:generations) { # calculate p, the probability of sampling the allele, based on s i &lt;- freq * Ne # number of individuals who currently carry the allele p &lt;- i*(1+s) / (Ne - i + i*(1+s)) # prob is now `p`, rather than `freq` new_freq &lt;- rbinom(n = 1, size = Ne, prob = p) / Ne freq_vector &lt;- c(freq_vector, new_freq) freq &lt;- new_freq } # convert vector of AFs into a tibble for plotting sim_results &lt;- tibble(afs = freq_vector, gen = 1:(generations+1)) # return the tibble of AFs, so that we can access the results return(sim_results) } Run and plot the simulation with selection: results &lt;- run_sim_selection(Ne = 10000, freq = 0.5, generations = 10000, s = -0.001) ggplot() + geom_line(data = results, aes(x = gen, y = afs)) + ylim(0, 1) + ylab(&quot;Allele frequency&quot;) + xlab(&quot;Generation&quot;) + ggtitle(&quot;Simulation with selection&quot;) + theme(plot.title = element_text(hjust = 0.5)) # to center the title Run and plot the simulation without selection: results &lt;- run_sim_selection(Ne = 10000, freq = 0.5, generations = 10000, s = 0) ggplot() + geom_line(data = results, aes(x = gen, y = afs)) + ylim(0, 1) + ylab(&quot;Allele frequency&quot;) + xlab(&quot;Generation&quot;) + ggtitle(&quot;Simulation without selection&quot;) + theme(plot.title = element_text(hjust = 0.5)) # to center the title We observe that selection tends to decrease the time it takes for an allele to either fix or go extinct, because it directionally biases the probability of sampling that allele. Decreasing the absolute value of the selection coefficient will make the simulation behave more like drift. "],["authors.html", "Authors", " Authors     Credits Names Pedagogy Instructor Rajiv McCoy Content Author Stephanie Yan Content Author Kate Weaver Website Template Jeff Leek &amp; The Johns Hopkins Data Science Lab Design Inspiration Ali Madooei &amp; JHU Data Structures Funding JHU Center for Educational Resources Techology Fellowship Grant   ## ─ Session info ─────────────────────────────────────────────────────────────── ## setting value ## version R version 4.0.2 (2020-06-22) ## os Ubuntu 20.04.5 LTS ## system x86_64, linux-gnu ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz Etc/UTC ## date 2023-04-11 ## ## ─ Packages ─────────────────────────────────────────────────────────────────── ## package * version date lib source ## admixr * 0.9.1 2020-07-03 [1] RSPM (R 4.0.2) ## ape 5.4-1 2020-08-13 [1] RSPM (R 4.0.2) ## aplot 0.0.6 2020-09-03 [1] RSPM (R 4.0.3) ## assertthat 0.2.1 2019-03-21 [1] RSPM (R 4.0.5) ## backports 1.1.10 2020-09-15 [1] RSPM (R 4.0.2) ## BiocManager 1.30.10 2019-11-16 [1] RSPM (R 4.0.0) ## blob 1.2.1 2020-01-20 [1] RSPM (R 4.0.3) ## bookdown 0.24 2023-03-28 [1] Github (rstudio/bookdown@88bc4ea) ## broom 0.7.1 2020-10-02 [1] RSPM (R 4.0.2) ## bslib 0.4.2 2022-12-16 [1] CRAN (R 4.0.2) ## cachem 1.0.7 2023-02-24 [1] CRAN (R 4.0.2) ## calibrate 1.7.7 2020-06-19 [1] RSPM (R 4.0.1) ## callr 3.5.0 2020-10-08 [1] RSPM (R 4.0.2) ## cellranger 1.1.0 2016-07-27 [1] RSPM (R 4.0.3) ## cli 3.6.1 2023-03-23 [1] CRAN (R 4.0.2) ## cluster 2.1.0 2019-06-19 [2] CRAN (R 4.0.2) ## colorspace 1.4-1 2019-03-18 [1] RSPM (R 4.0.0) ## crayon 1.3.4 2017-09-16 [1] RSPM (R 4.0.0) ## DBI 1.1.0 2019-12-15 [1] RSPM (R 4.0.0) ## dbplyr 1.4.4 2020-05-27 [1] RSPM (R 4.0.0) ## desc 1.2.0 2018-05-01 [1] RSPM (R 4.0.3) ## devtools 2.3.2 2020-09-18 [1] RSPM (R 4.0.3) ## digest 0.6.25 2020-02-23 [1] RSPM (R 4.0.0) ## dplyr * 1.0.2 2020-08-18 [1] RSPM (R 4.0.2) ## ellipsis 0.3.1 2020-05-15 [1] RSPM (R 4.0.3) ## evaluate 0.20 2023-01-17 [1] CRAN (R 4.0.2) ## fansi 0.4.1 2020-01-08 [1] RSPM (R 4.0.0) ## farver 2.0.3 2020-01-16 [1] RSPM (R 4.0.3) ## fastmap 1.1.1 2023-02-24 [1] CRAN (R 4.0.2) ## forcats * 0.5.0 2020-03-01 [1] RSPM (R 4.0.0) ## fs 1.5.0 2020-07-31 [1] RSPM (R 4.0.3) ## generics 0.0.2 2018-11-29 [1] RSPM (R 4.0.0) ## ggplot2 * 3.3.2 2020-06-19 [1] RSPM (R 4.0.1) ## ggtree * 2.4.2 2021-04-26 [1] Bioconductor ## glue 1.4.2 2020-08-27 [1] RSPM (R 4.0.5) ## gtable 0.3.0 2019-03-25 [1] RSPM (R 4.0.3) ## haven 2.3.1 2020-06-01 [1] RSPM (R 4.0.2) ## highr 0.8 2019-03-20 [1] RSPM (R 4.0.3) ## hms 0.5.3 2020-01-08 [1] RSPM (R 4.0.0) ## htmltools 0.5.5 2023-03-23 [1] CRAN (R 4.0.2) ## httr 1.4.2 2020-07-20 [1] RSPM (R 4.0.3) ## jquerylib 0.1.4 2021-04-26 [1] CRAN (R 4.0.2) ## jsonlite 1.7.1 2020-09-07 [1] RSPM (R 4.0.2) ## knitr 1.33 2023-03-28 [1] Github (yihui/knitr@a1052d1) ## labeling 0.3 2014-08-23 [1] RSPM (R 4.0.0) ## lattice 0.20-41 2020-04-02 [2] CRAN (R 4.0.2) ## lazyeval 0.2.2 2019-03-15 [1] RSPM (R 4.0.3) ## lifecycle 1.0.3 2022-10-07 [1] CRAN (R 4.0.2) ## lubridate 1.7.9 2020-06-08 [1] RSPM (R 4.0.2) ## magrittr 2.0.3 2022-03-30 [1] CRAN (R 4.0.2) ## MASS 7.3-51.6 2020-04-26 [2] CRAN (R 4.0.2) ## Matrix 1.2-18 2019-11-27 [2] CRAN (R 4.0.2) ## memoise 2.0.1 2021-11-26 [1] CRAN (R 4.0.2) ## memuse 4.1-0 2020-02-17 [1] RSPM (R 4.0.0) ## mgcv 1.8-31 2019-11-09 [2] CRAN (R 4.0.2) ## modelr 0.1.8 2020-05-19 [1] RSPM (R 4.0.3) ## munsell 0.5.0 2018-06-12 [1] RSPM (R 4.0.3) ## nlme 3.1-148 2020-05-24 [2] CRAN (R 4.0.2) ## ottrpal 1.0.1 2023-03-28 [1] Github (jhudsl/ottrpal@151e412) ## patchwork 1.0.1 2020-06-22 [1] RSPM (R 4.0.2) ## permute 0.9-5 2019-03-12 [1] RSPM (R 4.0.3) ## pillar 1.9.0 2023-03-22 [1] CRAN (R 4.0.2) ## pinfsc50 1.2.0 2020-06-03 [1] RSPM (R 4.0.0) ## pkgbuild 1.1.0 2020-07-13 [1] RSPM (R 4.0.2) ## pkgconfig 2.0.3 2019-09-22 [1] RSPM (R 4.0.3) ## pkgload 1.1.0 2020-05-29 [1] RSPM (R 4.0.3) ## prettyunits 1.1.1 2020-01-24 [1] RSPM (R 4.0.3) ## processx 3.4.4 2020-09-03 [1] RSPM (R 4.0.2) ## ps 1.4.0 2020-10-07 [1] RSPM (R 4.0.2) ## purrr * 0.3.4 2020-04-17 [1] RSPM (R 4.0.5) ## qqman * 0.1.4 2017-03-16 [1] RSPM (R 4.0.0) ## R6 2.4.1 2019-11-12 [1] RSPM (R 4.0.0) ## Rcpp 1.0.10 2023-01-22 [1] CRAN (R 4.0.2) ## readr * 1.4.0 2020-10-05 [1] RSPM (R 4.0.2) ## readxl 1.3.1 2019-03-13 [1] RSPM (R 4.0.2) ## remotes 2.2.0 2020-07-21 [1] RSPM (R 4.0.3) ## reprex 0.3.0 2019-05-16 [1] RSPM (R 4.0.0) ## rlang 1.1.0 2023-03-14 [1] CRAN (R 4.0.2) ## rmarkdown 2.10 2023-03-28 [1] Github (rstudio/rmarkdown@02d3c25) ## rprojroot 2.0.3 2022-04-02 [1] CRAN (R 4.0.2) ## rstudioapi 0.11 2020-02-07 [1] RSPM (R 4.0.0) ## rvcheck 0.1.8 2020-03-01 [1] RSPM (R 4.0.0) ## rvest 1.0.1 2023-03-28 [1] Github (tidyverse/rvest@4fe39fb) ## sass 0.4.5 2023-01-24 [1] CRAN (R 4.0.2) ## scales 1.1.1 2020-05-11 [1] RSPM (R 4.0.3) ## sessioninfo 1.1.1 2018-11-05 [1] RSPM (R 4.0.3) ## stringi 1.5.3 2020-09-09 [1] RSPM (R 4.0.3) ## stringr * 1.4.0 2019-02-10 [1] RSPM (R 4.0.3) ## testthat 3.0.1 2023-03-28 [1] Github (R-lib/testthat@e99155a) ## tibble * 3.2.1 2023-03-20 [1] CRAN (R 4.0.2) ## tidyr * 1.1.2 2020-08-27 [1] RSPM (R 4.0.2) ## tidyselect 1.1.0 2020-05-11 [1] RSPM (R 4.0.3) ## tidytree 0.3.3 2020-04-02 [1] RSPM (R 4.0.3) ## tidyverse * 1.3.0 2019-11-21 [1] RSPM (R 4.0.3) ## treeio 1.14.4 2021-04-26 [1] Bioconductor ## usethis 1.6.3 2020-09-17 [1] RSPM (R 4.0.2) ## utf8 1.1.4 2018-05-24 [1] RSPM (R 4.0.3) ## vcfR * 1.12.0 2020-09-01 [1] RSPM (R 4.0.2) ## vctrs 0.6.1 2023-03-22 [1] CRAN (R 4.0.2) ## vegan 2.5-6 2019-09-01 [1] RSPM (R 4.0.0) ## viridisLite 0.3.0 2018-02-01 [1] RSPM (R 4.0.3) ## withr 2.3.0 2020-09-22 [1] RSPM (R 4.0.2) ## xfun 0.26 2023-03-28 [1] Github (yihui/xfun@74c2a66) ## xml2 1.3.2 2020-04-23 [1] RSPM (R 4.0.3) ## yaml 2.2.1 2020-02-01 [1] RSPM (R 4.0.3) ## ## [1] /usr/local/lib/R/site-library ## [2] /usr/local/lib/R/library "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
